{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lecture 06: Majorization-Minimization and Expectation Maximization\n",
    "\n",
    "## Majorization-Minimization\n",
    "\n",
    "We say that a function $g:\\mathbb{R}^d\\times\\mathbb{R}^d\\rightarrow\\mathbb{R}$ **majorizes** a function $f:\\mathbb{R}^d\\rightarrow\\mathbb{R}$ if\n",
    "\n",
    "1. $g({\\bf x}, {\\bf x}^\\prime)\\geq f({\\bf x})$ for all ${\\bf x}, {\\bf x}^\\prime\\in \\mathbb{R}^d$, and\n",
    "2. $g({\\bf x}^\\prime, {\\bf x}^\\prime)=f({\\bf x}^\\prime)$ for all ${\\bf x}^\\prime\\in \\mathbb{R}^d$.\n",
    "\n",
    "Whenever we have a majorization relationship, we can define a **majorization-minimization** optimization procedure by intializing ${\\bf x}^{(0)}$ and computing the sequence\n",
    "\n",
    "$$\n",
    "{\\bf x}^{(k+1)} = \\arg\\min_{\\bf x} g({\\bf x}, {\\bf x}^{(k)}).\n",
    "$$\n",
    "\n",
    "We then have that\n",
    "\n",
    "$$\n",
    "f({\\bf x}^{(k+1)})\\leq g({\\bf x}^{(k+1)}, {\\bf x}^{(k)})\\leq g({\\bf x}^{(k)}, {\\bf x}^{(k)})=f({\\bf x}^{(k)})\n",
    "$$\n",
    "\n",
    "and it follows from the transitive property that $f({\\bf x}^{(k)})$ is a monotonically decreasing sequence of values.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Let $f(x)=x\\arctan(x)-\\log(1+x^2)/2$. Then\n",
    "\n",
    "$$\n",
    "g(x,x^\\prime) = f(x^\\prime) + \\arctan(x^\\prime)(x-x^\\prime) + \\frac{1}{2}(x-x^\\prime)^2\n",
    "$$\n",
    "\n",
    "majorizes $f$. It is easy to see that $g(x^\\prime,x^\\prime)=f(x^\\prime)$ for all $x^\\prime$. On the other hand, Taylor's theorem gives a $\\xi$ between $x$ and $x^\\prime$ such that\n",
    "\n",
    "$$\n",
    "f(x) = f(x^\\prime) + f^\\prime(x^\\prime)(x-x^\\prime) + \\frac{1}{2} f^{\\prime\\prime}(\\xi)(x-x^\\prime)^2=f(x^\\prime) + \\arctan(x^\\prime)(x-x^\\prime) + \\frac{1}{2} \\frac{1}{1+\\xi^2}(x-x^\\prime)^2\\leq f(x^\\prime) + \\arctan(x^\\prime)(x-x^\\prime) + \\frac{1}{2} (x-x^\\prime)^2 = g(x,x^\\prime).\n",
    "$$\n",
    "\n",
    "Noting that $g(x,x^\\prime)$ is convex as a function of $x$, we have that $0=\\frac{d}{dx} g(x, x^\\prime) = \\arctan(x^\\prime) + (x-x^\\prime)$ is necessary and sufficient for optimality. Thus, $x = x^\\prime - \\arctan(x^\\prime)$ is the only minimizer of $g(x, x^\\prime)$, so the iterates in majorization-minimization will be\n",
    "\n",
    "$$\n",
    "x^{(k+1)} = x^{(k)} - \\arctan(x^{(k)}).\n",
    "$$\n",
    "\n",
    "## A Large Class of Examples\n",
    "\n",
    "\n",
    "Based on the reasoning in this example, if $f\\in C^2(\\mathbb{R})$ with $\\vert f^{\\prime\\prime}(x)\\vert\\leq C$ for all $x\\in\\mathbb{R}$, we have that\n",
    "\n",
    "$$\n",
    "g(x,x^\\prime) = f(x^\\prime) + f^\\prime(x^\\prime)(x-x^\\prime) + \\frac{C}{2}(x-x^\\prime)^2.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "The generalization to higher dimensions requires us to define the **operator** or **spectral** norm of a square matrix:\n",
    "\n",
    "$$\n",
    "\\Vert A\\Vert_{\\text{op}} = \\min_{\\Vert {\\bf u}\\Vert=1} {\\bf u}^T A {\\bf u}.\n",
    "$$\n",
    "\n",
    "We then get that, if $f\\in C^2(\\mathbb{R}^d)$ and $\\Vert \\nabla^2 f({\\bf x})\\Vert_{\\text{op}}\\leq C$ for all ${\\bf x}\\in\\mathbb{R}^d$, then \n",
    "\n",
    "$$\n",
    "g({\\bf x}, {\\bf x}^\\prime)= f({\\bf x}^\\prime) + \\nabla f({\\bf x}^\\prime)^T({\\bf x}-{\\bf x}^\\prime) + \\frac{C}{2}\\Vert {\\bf x}-{\\bf x}^\\prime\\Vert^2.\n",
    "$$\n",
    "\n",
    "The reason we get this generalization is that Taylor's theorem extends to higher dimensions. In particular, for $f\\in C^2(\\mathbb{R}^d)$, there is a $\\xi$ on the *line segment connecting* ${\\bf x}$ and ${\\bf x}^\\prime$ with\n",
    "\n",
    "$$\n",
    "f({\\bf x}) = f({\\bf x}^\\prime) + \\nabla f({\\bf x}^\\prime)^T({\\bf x}-{\\bf x}^\\prime) + \\frac{1}{2}({\\bf x}-{\\bf x}^\\prime)^T\\nabla^2 f(\\xi)({\\bf x}-{\\bf x}^\\prime).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation-Maximization\n",
    "\n",
    "Expectation-Maximization is a technique for fitting **latent mixture models** using (essentially) majorization minimization.\n",
    "\n",
    "### Latent mixture models\n",
    "\n",
    "A latent mixture model is a random variable $X$ whose probability density function has the form\n",
    "\n",
    "$$\n",
    "p(X=x) = \\sum_{i=1}^k p(X=x, C=i).\n",
    "$$\n",
    "\n",
    "That is, the density function is a marginal of a joint probability density over random variables $X$ and $C$, where $C$ takes values in the finite set $\\{1,\\ldots, k\\}$. We call $C$ the **latent variable** because it is generally never observed. Using the multiplicative law of probability, we have\n",
    "\n",
    "$$\n",
    "p(X=x) = \\sum_{i=1}^k p(X=x\\vert C=i) p(C=i),\n",
    "$$\n",
    "\n",
    "which tells us that $p(X=x)$ is a weighted mixture of the conditional densities $p(X=x\\vert C=i)$, with weights given by $p(C=i)$. It therefore makes sense to call this a *latent mixture model*.\n",
    "\n",
    "This representation of $p(X)$ also indicates that $X$ can be generated in a *hierarchical manner*:\n",
    "\n",
    "1. Draw $c$ according to the density $p(C)$\n",
    "2. Draw $X$ according to the conditional density $p(X\\vert C=c)$\n",
    "\n",
    "### Fitting parametric latent mixture models\n",
    "\n",
    "**Density estimation** seeks to estimate the true density of a random variable $X$. That is, we are given data $\\{ X_n\\}_{n=1}^N$ and we use this to estimate $\\widehat{p}(X)$ which is suitably close to the true density $p(X)$. One way to do this is by setting up a **parameteric latent mixture model**. To do this we restrict our estimate to densities of the form\n",
    "\n",
    "$$\n",
    "p(X=x; \\theta_1,\\ldots, \\theta_k) = \\sum_{i=1}^k p(X=x;\\theta_i)p(C=i)\n",
    "$$\n",
    "\n",
    "where $p(X=x;\\theta)$ is a density that depends on the parameter $\\theta$, and $k$ is a fixed hyperparameter. For example, we could use the Gaussian parametric family\n",
    "\n",
    "$$\n",
    "p(X=x; \\theta) = \\frac{1}{\\sqrt{2\\pi}} e^{-(x-\\theta)^2/2}\\text{ for }\\theta\\in\\mathbb{R}\n",
    "$$\n",
    "\n",
    "and then\n",
    "\n",
    "$$\n",
    "p(X=x; \\theta_1,\\ldots, \\theta_k) = \\sum_{i=1}^k \\frac{1}{\\sqrt{2\\pi}} e^{-(x-\\theta_i)^2/2}p(C=i).\n",
    "$$\n",
    "\n",
    "Now, we also don't know $\\alpha_i=p(C=i)$, so this must be estimated as well. Thus, a simple **Gaussian mixture model** would look like\n",
    "\n",
    "$$\n",
    "p(X=x; \\theta_1,\\ldots, \\theta_k,\\alpha_1,\\ldots,\\alpha_k) = \\sum_{i=1}^k \\frac{\\alpha_i}{\\sqrt{2\\pi}} e^{-(x-\\theta_i)^2/2}.\n",
    "$$\n",
    "\n",
    "where $\\theta_i,\\alpha_i\\in \\mathbb{R}$, $\\alpha_i\\geq0$, and $\\sum_{i=1}^k\\alpha_i=1$. \n",
    "\n",
    "A more flexible Gaussian mixture model would allow us to also fit the variance inside of the *mixture components*. Thus, the general Gaussian mixture model for a 1D random variable is given by\n",
    "\n",
    "$$\n",
    "p(X=x; \\mu_1,\\ldots, \\mu_k,\\sigma_1,\\ldots,\\sigma_k,\\alpha_1,\\ldots,\\alpha_k) = \\sum_{i=1}^k \\frac{\\alpha_i}{\\sqrt{2\\pi\\sigma_i^2}} e^{-\\frac{1}{2\\sigma_i^2}(x-\\mu_i)^2}.\n",
    "$$\n",
    "\n",
    "where $\\mu_i,\\sigma_i,\\alpha_i\\in\\mathbb{R}$, $\\sigma_i>0$, $\\alpha_i\\geq 0$, and $\\sum_{i=1}^k\\alpha_i=1$.\n",
    "\n",
    "In particular, if we wanted to try to approximate a *bimodal distribution*, we might try to fit\n",
    "\n",
    "$$\n",
    "p(X=x;\\mu_1,\\mu_2,\\sigma_1,\\sigma_2,\\alpha_1,\\alpha_2)= \\alpha_1 p(X=x;\\mu_1,\\sigma_1) + \\alpha_2 p(X-x;\\mu_2,\\sigma_2)=\\frac{\\alpha_1}{\\sqrt{2\\pi\\sigma_1^2}}e^{-\\frac{1}{2\\sigma_1^2}(x-\\mu_1)^2} + \\frac{\\alpha_2}{\\sqrt{2\\pi\\sigma_2^2}}e^{-\\frac{1}{2\\sigma_2^2}(x-\\mu_2)^2}\n",
    "$$\n",
    "\n",
    "to a dataset $\\{x_n\\}_{n=1}^N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood for parameter estimation in the two-component Gaussian mixture model\n",
    "\n",
    "The most straightforward way to fit this parametric model to data is to form a likelihood to maximize:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mu_1,\\mu_2,\\sigma_1,\\sigma_2,\\alpha_1,\\alpha_2) = \\prod_{n=1}^N p(X=x_n;\\mu_1,\\mu_2,\\sigma_1,\\sigma_2,\\alpha_1,\\alpha_2).\n",
    "$$\n",
    "\n",
    "This is converted to a negative log-likelihood \n",
    "\n",
    "$$\n",
    "\\ell(\\mu_1,\\mu_2,\\sigma_1,\\sigma_2,\\alpha_1,\\alpha_2) = -\\sum_{n=1}^N\\log p(X=x_n;\\mu_1,\\mu_2,\\sigma_1,\\sigma_2,\\alpha_1,\\alpha_2)= -\\sum_{n=1}^N\\log \\left(\\alpha_1p(X=x_n;\\mu_1,\\sigma_1) + \\alpha_2 p(X=x_n;\\mu_2,\\sigma_2)\\right)\n",
    "$$\n",
    "\n",
    "To find a function which majorizes this sum, we first find a function $g_x(\\theta,\\theta^\\prime)$ which majorizes\n",
    "$$\n",
    "f_x(\\theta)=-\\log\\left(\\alpha_1p(X=x;\\mu_1,\\sigma_1) + \\alpha_2 p(X=x;\\mu_2,\\sigma_2)\\right)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\theta = (\\mu_1,\\mu_2,\\sigma_1,\\sigma_2,\\alpha_1,\\alpha_2)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\theta^\\prime = (\\mu_1^\\prime,\\mu_2^\\prime,\\sigma_1^\\prime,\\sigma_2^\\prime,\\alpha_1^\\prime,\\alpha_2^\\prime).\n",
    "$$\n",
    "We would then have that $\\ell(\\theta)=\\sum_{n=1}^N f_{x_i}(\\theta)$ is majorized by $g_{x_i}(\\theta,\\theta^\\prime)$.\n",
    "\n",
    "We set\n",
    "$$\n",
    "q_i(\\theta) = \\alpha_i p(X=x;\\mu_i,\\sigma_i)\n",
    "$$\n",
    "for $i=1,2$. We claim that\n",
    "$$\n",
    "g_x(\\theta, \\theta^\\prime) = -\\frac{q_1(\\theta^\\prime)}{q_1(\\theta^\\prime)+q_2(\\theta^\\prime)}\\log\\left(\\frac{q_1(\\theta)}{q_1(\\theta^\\prime)}(q_1(\\theta^\\prime)+q_2(\\theta^\\prime))\\right) -\\frac{q_2(\\theta^\\prime)}{q_1(\\theta^\\prime)+q_2(\\theta^\\prime)}\\log\\left(\\frac{q_2(\\theta)}{q_2(\\theta^\\prime)}(q_1(\\theta^\\prime)+q_2(\\theta^\\prime))\\right)\n",
    "$$\n",
    "majorizes $f_x(\\theta)$. To simplify a little further, we note that\n",
    "$$\n",
    "\\frac{q_i(\\theta^\\prime)}{q_1(\\theta^\\prime)+q_2(\\theta^\\prime)}\n",
    "$$\n",
    "is the conditional probility of that $x$ was drawn from component $i$, $p(C=i\\vert X=x;\\theta^\\prime)$. In particular, $p(C=1\\vert X=x;\\theta^\\prime)+p(C=2\\vert X=x;\\theta^\\prime)=1$. Thus,\n",
    "\n",
    "$$\n",
    "g_x(\\theta, \\theta^\\prime) = -p(C=1\\vert X=x;\\theta^\\prime)\\log\\left(\\frac{q_1(\\theta)}{p(C=1\\vert X=x;\\theta^\\prime)}\\right) -p(C=2\\vert X=x;\\theta^\\prime)\\log\\left(\\frac{q_2(\\theta)}{p(C=2\\vert X=x;\\theta^\\prime)}\\right)\n",
    "$$\n",
    "\n",
    "We first note that\n",
    "\n",
    "$$\n",
    "g_x(\\theta^\\prime,\\theta^\\prime)= \\frac{q_1(\\theta^\\prime)}{q_1(\\theta^\\prime)+q_2(\\theta^\\prime)}\\log\\left(\\frac{q_1(\\theta^\\prime)}{q_1(\\theta^\\prime)}(q_1(\\theta^\\prime)+q_2(\\theta^\\prime))\\right) -\\frac{q_2(\\theta^\\prime)}{q_1(\\theta^\\prime)+q_2(\\theta^\\prime)}\\log\\left(\\frac{q_2(\\theta^\\prime)}{q_2(\\theta^\\prime)}(q_1(\\theta^\\prime)+q_2(\\theta^\\prime))\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "g_x(\\theta^\\prime,\\theta^\\prime)=\\frac{q_1(\\theta^\\prime)}{q_1(\\theta^\\prime)+q_2(\\theta^\\prime)}\\log\\left(q_1(\\theta^\\prime)+q_2(\\theta^\\prime)\\right) -\\frac{q_2(\\theta^\\prime)}{q_1(\\theta^\\prime)+q_2(\\theta^\\prime)}\\log\\left(q_1(\\theta^\\prime)+q_2(\\theta^\\prime)\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "g_x(\\theta^\\prime,\\theta^\\prime)=-(p(C=1\\vert X=x;\\theta^\\prime)+p(C=2\\vert X=x;\\theta^\\prime))\\log\\left(q_1(\\theta^\\prime)+q_2(\\theta^\\prime)\\right)=-\\log\\left(q_1(\\theta^\\prime)+q_2(\\theta^\\prime)\\right)=f_x(\\theta^\\prime).\n",
    "$$\n",
    "\n",
    "To show majorization, we see that\n",
    "\n",
    "$$\n",
    "f_x(\\theta)=-\\log\\left(q_1(\\theta)+q_2(\\theta)\\right)=-\\log\\left(p(C=1\\vert X=x;\\theta^\\prime)\\frac{q_1(\\theta)}{p(C=1\\vert X=x;\\theta^\\prime)}+p(C=2\\vert X=x;\\theta^\\prime)\\frac{q_2(\\theta)}{p(C=2\\vert X=x;\\theta^\\prime)}\\right).\n",
    "$$\n",
    "\n",
    "Noting that $-\\log$ is a convex function, we have\n",
    "\n",
    "$$\n",
    "f_x(\\theta)\\leq -p(C=1\\vert X=x;\\theta^\\prime)\\log\\left(\\frac{q_1(\\theta)}{p(C=1\\vert X=x;\\theta^\\prime)}\\right) -p(C=2\\vert X=x;\\theta^\\prime)\\log\\left(\\frac{q_2(\\theta)}{p(C=2\\vert X=x;\\theta^\\prime)}\\right)=g_x(\\theta,\\theta^\\prime).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Expectation-Maximization \n",
    "\n",
    "While majorization-minimization indicates how to proceed to begin minimizing $\\ell(\\theta)$, we now explain why this algorithm is equivalent to the **expectation-maximization** procedure. First, we introduce a likelihood function where the latent variables $c_n$ are known. That is, suppose we have the data $\\mathcal{X}=\\{x_n\\}_{n=1}^N$ and associated latent variables $\\mathcal{C}=\\{i_n\\}_{n=1}^N$. Then the likelihood of the parameter $\\theta$ given this data is\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta; \\mathcal{X}, \\mathcal{C}) = \\prod_{n=1}^N \\alpha_{i_n}p(X=x_n; \\mu_{i_n}, \\sigma_{i_n})\n",
    "$$\n",
    "\n",
    "and the negative log-likelihood is given by\n",
    "\n",
    "\n",
    "$$\n",
    "\\ell(\\theta; \\mathcal{X}, \\mathcal{C}) = -\\sum_{n=1}^N \\log\\left(\\alpha_{i_n}p(X=x_n; \\mu_{i_n}, \\sigma_{i_n})\\right)\n",
    "$$\n",
    "\n",
    "For our mixture of two components, the expectation-maximization algorithm has the following form:\n",
    "\n",
    "1. **E Step**: Compute $p(C=i|X=x_n;\\theta^{(k)})$ for $i=1,2$ and $n=1,\\ldots, N$ to form the conditional expectation\n",
    "\n",
    "$$\n",
    "Q(\\theta\\vert\\theta^{(k)})= \\mathbb{E}_{\\mathcal{C}\\vert \\mathcal{X};\\theta^{(k)}} \\log\\mathcal{L}(\\theta;\\mathcal{X}, \\mathcal{C}) = \\mathbb{E}_{\\mathcal{C}\\vert \\mathcal{X};\\theta^{(k)}}\\sum_{n=1}^N \\log\\left(\\alpha_{i_n}p(X=x_n; \\mu_{i_n}, \\sigma_{i_n})\\right) = \\sum_{n=1}^N \\mathbb{E}_{\\mathcal{C}\\vert \\mathcal{X};\\theta^{(k)}}\\log\\left(\\alpha_{i_n}p(X=x_n; \\mu_{i_n}, \\sigma_{i_n})\\right),\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "Q(\\theta\\vert\\theta^{(k)})= \\sum_{n=1}^N p(C=1\\vert X=x_n;\\theta^{(k)})\\log\\left(\\alpha_{1}p(X=x_n; \\mu_{1}, \\sigma_{1})\\right) + p(C=2\\vert X=x_n;\\theta^{(k)})\\log\\left(\\alpha_{2}p(X=x_n; \\mu_{2}, \\sigma_{2})\\right),\n",
    "$$\n",
    "\n",
    "which becomes\n",
    "\n",
    "$$\n",
    "Q(\\theta\\vert\\theta^{(k)})= \\sum_{n=1}^N p(C=1\\vert X=x_n;\\theta^{(k)})\\log\\left(\\alpha_{1}p(X=x_n; \\mu_{1}, \\sigma_{1})\\right) + p(C=2\\vert X=x_n;\\theta^{(k)})\\log\\left(\\alpha_{2}p(X=x_n; \\mu_{2}, \\sigma_{2})\\right),\n",
    "$$\n",
    "\n",
    "2. **M step**: Solve $\\theta^{(k+1)} = \\arg\\max_{\\theta} Q(\\theta\\vert \\theta^{(k)})$. Note that this program is equivalent to the program\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\sum_{n=1}^N g_{x_n}(\\theta, \\theta^{(k)})\n",
    "$$\n",
    "\n",
    "since\n",
    "\n",
    "$$\n",
    "g_{x}(\\theta,\\theta^{(k)})=-p(C=1\\vert X=x;\\theta^{(k)})\\log\\left(\\alpha_{1}p(X=x; \\mu_{1}, \\sigma_{1})\\right) - p(C=2\\vert X=x;\\theta^{(k)})\\log\\left(\\alpha_{2}p(X=x; \\mu_{2}, \\sigma_{2})\\right)\\\\ + p(C=1\\vert X=x;\\theta^{(k)})\\log\\left(p(C=1\\vert X=x;\\theta^{(k)})\\right) + p(C=2\\vert X=x;\\theta^{(k)})\\log\\left(p(C=2\\vert X=x;\\theta^{(k)})\\right),\n",
    "$$\n",
    "\n",
    "and hence\n",
    "\n",
    "$$\n",
    "\\sum_{n=1}^N g_{x_n}(\\theta, \\theta^{(k)}) = -Q(\\theta\\vert\\theta^{(k)})\n",
    "+\\sum_{n=1}^N p(C=1\\vert X=x;\\theta^{(k)})\\log\\left(p(C=1\\vert X=x;\\theta^{(k)})\\right) + p(C=2\\vert X=x;\\theta^{(k)})\\log\\left(p(C=2\\vert X=x;\\theta^{(k)})\\right).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "\n",
    "First, it is relatively simple to compute the quantities\n",
    "\n",
    "$$\n",
    "q_{i,n}=p(C=i\\vert X=x_n;\\theta^{(k)}) = \\frac{\\alpha_i p(X=x_n;\\mu_i^{(k)},\\sigma_i^{(k)})}{\\alpha_1 p(X=x_n;\\mu_1^{(k)},\\sigma_1^{(k)})+\\alpha_2 p(X=x_n;\\mu_2^{(k)},\\sigma_2^{(k)})}\n",
    "$$\n",
    "\n",
    "Next, we observe that\n",
    "\n",
    "$$\n",
    "\\log\\left(\\alpha_i p(X=x_n; \\mu_i,\\sigma_i)\\right)=\\log\\left(\\alpha_i \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}}e^{-\\frac{(x_n-\\mu_i)^2}{2\\sigma_i^2}}\\right)=\\log(\\alpha_i)-\\frac{1}{2}\\log(2\\pi)-\\log(\\sigma_i) -\\frac{(x_n-\\mu_i)^2}{2\\sigma_i^2},\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "\\sum_{n=1}^N g_{x_n}(\\theta,\\theta^{(k)}) = \\sum_{n=1}^N q_{1, n}\\log(\\alpha_1)+\\sum_{n=1}^N q_{2, n}\\log(\\alpha_2)-\\sum_{n=1}^N q_{1, n}\\frac{1}{2}\\log(2\\pi)-\\sum_{n=1}^N q_{2, n}\\frac{1}{2}\\log(2\\pi)-\\sum_{n=1}^N q_{1, n}\\left(\\log(\\sigma_1) +\\frac{(x_n-\\mu_1)^2}{2\\sigma_1^2}\\right)-\\sum_{n=1}^N q_{2, n}\\left(\\log(\\sigma_2) +\\frac{(x_n-\\mu_2)^2}{2\\sigma_2^2}\\right).\n",
    "$$\n",
    "\n",
    "Minimizing subject to the constraint $\\alpha_1+\\alpha_2$ gives us a Lagrange multiplier such that\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\frac{1}{\\alpha_1}\\sum_{n=1}^N q_{1, n}\\\\\n",
    "\\frac{1}{\\alpha_2}\\sum_{n=1}^N q_{2, n}\n",
    "\\end{pmatrix}=\\lambda\\begin{pmatrix} 1\\\\ 1\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and therefore $\\alpha_1^{(k+1)} = \\frac{1}{N} \\sum_{n=1}^N q_{1, n}$ and $\\alpha_2^{(k+1)} = \\frac{1}{N}\\sum_{n=1}^N q_{2, n}$.\n",
    "\n",
    "Taking the gradient with respect to the unconstrained variables $\\mu_1$ and $\\mu_2$, we obtain the necessary conditions\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sigma_1^2}\\sum_{n=1}^N q_{1, n} (x_n - \\mu_1)=0\\text{ and } \\frac{1}{\\sigma_2^2}\\sum_{n=1}^N q_{2, n} (x_n - \\mu_2)=0\n",
    "$$\n",
    "\n",
    "so that \n",
    "\n",
    "$$\n",
    "\\mu_1^{(k+1)}=\\frac{1}{\\sum_{n=1}^N q_{1, n}}\\sum_{n=1}^N q_{1, n} x_n\\text{ and } \\mu_2^{(k+1)}=\\frac{1}{\\sum_{n=1}^N q_{2, n}}\\sum_{n=1}^N q_{2, n} x_n.\n",
    "$$\n",
    "\n",
    "Finally, taking the gradient with respect to $\\sigma_1$ and $\\sigma_2$ give the conditions\n",
    "\n",
    "$$\n",
    "-\\sum_{n=1}^N q_{i, n}\\left(\\frac{1}{\\sigma_i} -\\frac{(x_n-\\mu_i^{(k+1)})^2}{\\sigma_i^3}\\right)=0\\text{ for }i=1,2\n",
    "$$\n",
    "\n",
    "which reduces to\n",
    "\n",
    "$$\n",
    "\\sigma_i^2 = \\left(\\frac{1}{\\sum_{n=1}^N q_{i, n}}\\sum_{n=1}^N q_{i, n} x_n^2\\right) - \\left(\\mu_i^{(k+1)}\\right)^2\\text{ for }i=1,2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkvlOKO5r348AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ca449e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gauss_1D_pdf(x, mu,sig):\n",
    "    return np.exp(-(x-mu)**2/(2*sig**2))/np.sqrt(2*np.pi*sig**2)\n",
    "\n",
    "def two_gauss_mix_pdf(th):\n",
    "    def f(x):\n",
    "        return th[0] * gauss_1D_pdf(x, th[1], th[2]) + th[3] * gauss_1D_pdf(x, th[4], th[5])\n",
    "    return f\n",
    "    \n",
    "def rand_two_gauss_mix(n, th):\n",
    "    latent = rd.choice(2, n, p=th[[0,3]])\n",
    "    x = rd.randn(n)\n",
    "    return (1-latent)*(th[2]*x + th[1]) + latent*(th[5]*x + th[4])\n",
    "    \n",
    "def estep(x, th):\n",
    "    comp_1_likelihoods = th[0] * gauss_1D_pdf(x, th[1], th[2])\n",
    "    comp_2_likelihoods = th[3] * gauss_1D_pdf(x, th[4], th[5])\n",
    "    normalizations = comp_1_likelihoods + comp_2_likelihoods\n",
    "    cond_probs_1 = comp_1_likelihoods / normalizations\n",
    "    cond_probs_2 = comp_2_likelihoods / normalizations\n",
    "    return np.vstack([cond_probs_1, cond_probs_2])\n",
    "\n",
    "def mstep(th, x, cond_probs):\n",
    "    th_new = np.zeros(6)\n",
    "    sum_cond_probs = np.sum(cond_probs, axis = 1)\n",
    "    th_new[[0, 3]] = sum_cond_probs / x.size\n",
    "    th_new[[1, 4]] = np.sum(cond_probs * x, axis = 1) / sum_cond_probs\n",
    "    th_new[[2, 5]] = np.sqrt((np.sum(cond_probs * x**2, axis=1) / sum_cond_probs) - th_new[[1, 4]]**2)\n",
    "    return th_new\n",
    "\n",
    "rd.seed(1234)\n",
    "\n",
    "theta_true = np.array([0.25, 0, 1, 0.75, 4, 1])\n",
    "\n",
    "num_samples = 400\n",
    "theta_init = [0.5, -1, 2, 0.5, 1, 2]\n",
    "theta = theta_init\n",
    "x = rand_two_gauss_mix(num_samples, theta_true)\n",
    "\n",
    "plt.scatter(x, np.zeros(num_samples))\n",
    "t = np.linspace(np.min(x), np.max(x), 100)\n",
    "f = two_gauss_mix_pdf(theta_true)\n",
    "plt.plot(t, f(t))\n",
    "\n",
    "iters = 400\n",
    "for it in range(iters):\n",
    "    f_est = two_gauss_mix_pdf(theta)\n",
    "    \n",
    "    cond_probs = estep(x, theta)\n",
    "    #print(cond_probs)\n",
    "    theta = mstep(theta, x, cond_probs)\n",
    "    #print(theta)\n",
    "\n",
    "plt.plot(t, f_est(t))\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Group Problems\n",
    "\n",
    "1. Groups 1 and 2: Derive the EM steps for a mixture of thee Gaussians\n",
    "2. Groups 3 and 4: Derive the EM steps for a mixture of two Gaussians in 2D\n",
    "3. Groups 5 and 6: Derive the EM steps for a mixture of two Cauchy random variables: \n",
    "$$\n",
    "p(X=x;\\mu,\\gamma) = \\frac{1}{\\pi\\gamma\\left(1+\\left(\\frac{(x-\\mu)}{\\gamma}\\right)^2\\right)}\n",
    "$$\n",
    "4. Groups 7 and 8: Derive the EM steps for approximating the joint probability table $P=(p_{i, j})\\in M_{m, n}$ with $p_{i, j}\\geq 0$ and $\\sum_i\\sum_j p_{i, j}=0$ using a latent mixture of two independent components: $\\alpha_1 {\\bf p}_1{\\bf q}_1^T + \\alpha_2 {\\bf p}_2{\\bf q}_2^T$ where $\\alpha_i\\in\\mathbb{R}$, and ${\\bf p}_i\\in\\mathbb{R}^m$, ${\\bf q}_i\\in\\mathbb{R}^n$ for $i=1, 2$ are **probability vectors**. That is, they have non-negative entries and their entries sum to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This code imports numpy packages and allows us to pass data from python to global javascript\n",
    "objects. It was developed by znah@github\n",
    "'''\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import HTML, Javascript, display\n",
    "\n",
    "def json_numpy_serializer(o):\n",
    "    if isinstance(o, np.ndarray):\n",
    "        return o.tolist()\n",
    "    raise TypeError(\"{} of type {} is not JSON serializable\".format(repr(o), type(o)))\n",
    "\n",
    "def jsglobal(**params):\n",
    "    code = [];\n",
    "    for name, value in params.items():\n",
    "        jsdata = json.dumps(value, default=json_numpy_serializer)\n",
    "        code.append(\"window.{}={};\".format(name, jsdata))\n",
    "    display(Javascript(\"\\n\".join(code)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "// Loading the compiled MathBox bundle.\n",
       "require.config({\n",
       "    //baseUrl:'', paths: {mathBox: '../../tree/static/mathbox/build/mathbox-bundle'}\n",
       "    // online compilation\n",
       "    baseUrl: '', paths: {mathBox: '../static/mathbox/build/mathbox-bundle'}\n",
       "    // online compilation without local library-- remove baseUrl\n",
       "    //paths: {mathBox: '//cdn.rawgit.com/unconed/mathbox/eaeb8e15/build/mathbox-bundle'}\n",
       "});\n",
       "\n",
       "// Minified graphing functions\n",
       "\n",
       "window.with_mathbox=function(element,func){require(['mathBox'],function(){var mathbox=mathBox({plugins:['core','controls','cursor','mathbox'],controls:{klass:THREE.OrbitControls},mathbox:{inspect:!1},element:element[0],loop:{start:!1},});var three=mathbox.three;three.renderer.setClearColor(new THREE.Color(0xFFFFFF),1.0);three.camera.position.set(-1,1,2);three.controls.noKeys=!0;three.element.style.height=\"400px\";three.element.style.width=\"100%\";function isInViewport(element){var rect=element.getBoundingClientRect();var html=document.documentElement;var w=window.innerWidth||html.clientWidth;var h=window.innerHeight||html.clientHeight;return rect.top<h&&rect.left<w&&rect.bottom>0&&rect.right>0}\n",
       "var intervalId=setInterval(function(){if(three.element.offsetParent===null){clearInterval(intervalId);three.destroy();return}\n",
       "var visible=isInViewport(three.canvas);if(three.Loop.running!=visible){visible?three.Loop.start():three.Loop.stop()}},100);func(mathbox);window.dispatchEvent(new Event('resize'))})};window.plotGraph=function(mathbox,f,xlabel='x',ylabel='y',zlabel='f(x,y)',rng=[[-3,3],[-5,5],[-3,3]]){var view=mathbox.cartesian({range:rng,scale:[1,1,1]},{rotation:(t)=>[0,t*0.02,0]}).grid({axes:[1,3]})\n",
       "view.area({id:'yaxis',width:1,height:1,axes:[1,3],expr:function(emit,x,y,i,j){emit(4,0,0);emit(0,0,0)},items:2,channels:3,}).text({font:'Helvetica',style:'bold',width:16,height:5,depth:2,expr:function(emit,i,j,k,time){emit(ylabel)},}).label({color:'#000000',snap:!1,outline:2,size:24,offset:[0,-32],depth:.5,zIndex:1});view.vector({points:'#yaxis',color:0x000000,width:9,start:!0});view.area({id:'xaxis',width:1,height:1,axes:[1,3],expr:function(emit,x,y,i,j){emit(0,0,4);emit(0,0,0)},items:2,channels:3,}).text({font:'Helvetica',style:'bold',width:16,height:5,depth:2,expr:function(emit,i,j,k,time){emit(xlabel)},}).label({color:'#000000',snap:!1,outline:2,size:24,offset:[0,-32],depth:.5,zIndex:1,});view.vector({points:'#xaxis',color:0x000000,width:9,start:!0,});view.area({id:'zaxis',width:1,height:1,axes:[1,3],expr:function(emit,x,y,i,j){emit(0,4,0);emit(0,0,0)},items:2,channels:3,}).text({font:'Helvetica',style:'bold',width:16,height:5,depth:2,expr:function(emit,i,j,k,time){emit(zlabel)},}).label({color:'#000000',snap:!1,outline:2,size:24,offset:[0,-32],depth:.5,zIndex:1,});view.vector({points:'#zaxis',color:0x000000,width:9,start:!0,});var graph=view.area({id:'graph',width:64,height:64,axes:[1,3],expr:function(emit,y,x,i,j){emit(y,f(x,y),x)},items:1,channels:3,});view.surface({shaded:!0,lineX:!0,lineY:!0,points:graph,color:0x0000FF,width:1,});return view};window.addSegment=function(view,p0,p1,col){view.array({width:128,expr:function(emit,i,time){var b=i/128;var a=1-b;emit(a*p0[1]+b*p1[1],a*p0[2]+b*p1[2],a*p0[0]+b*p1[0])},channels:3,});view.line({color:col,width:10,size:2.5,stroke:'dotted',start:!1,end:!1,})};window.addPoint=function(view,p,col,label){view.array({width:4,items:2,channels:3,expr:function(emit,i,t){emit(p[1],p[2],p[0])},}).point({color:col,points:'<',size:15,depth:.5,zBias:50,}).text({font:'Helvetica',style:'bold',width:16,height:5,depth:2,expr:function(emit,i,j,k,time){emit(label)},}).label({color:col,snap:!1,outline:2,size:24,offset:[0,-32],depth:.5,zIndex:1,})};window.addCurve=function(view,ab,x,y,z,col){view.array({width:128,expr:function(emit,i,time){var t=(ab[1]-ab[0])*(i/128)+ab[0];emit(y(t),z(t),x(t))},channels:3,});view.line({color:col,width:20,size:2.5,start:!0,end:!0,})};window.addClosedCurve=function(view,ab,x,y,z,col){view.array({width:128,expr:function(emit,i,time){var t=(ab[1]-ab[0])*(i/128)+ab[0];emit(y(t),z(t),x(t))},channels:3,});view.line({color:col,width:20,size:2.5,start:!1,end:!1,})};window.addSurface=function(view,ab,cd,x,y,z,col,opa){view.matrix({width:64,height:64,expr:function(emit,i,j,time){var p=(ab[1]-ab[0])*(i/64)+ab[0];var q=(cd[1]-cd[0])*(j/64)+cd[0];emit(y(p,q),z(p,q),x(p,q))},items:1,channels:3}).surface({shaded:!0,lineX:!1,lineY:!1,color:col,width:1,opacity:opa})}\n",
       "window.addSequence=function(view,seq,col){var idx=0;var d=new Date();var start=d.getTime();view.array({width:1,expr:function(emit,i,time){var nd=new Date();var now=nd.getTime();if(1000<now-start){idx=idx+1;if(seq.length<=idx){idx=0}\n",
       "start=now}\n",
       "emit(seq[idx][1],seq[idx][2],seq[idx][0])},items:1,channels:3}).point({color:col,points:'<',size:15,depth:.5,zBias:50,})}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "// Loading the compiled MathBox bundle.\n",
    "require.config({\n",
    "    //baseUrl:'', paths: {mathBox: '../../tree/static/mathbox/build/mathbox-bundle'}\n",
    "    // online compilation\n",
    "    baseUrl: '', paths: {mathBox: '../static/mathbox/build/mathbox-bundle'}\n",
    "    // online compilation without local library-- remove baseUrl\n",
    "    //paths: {mathBox: '//cdn.rawgit.com/unconed/mathbox/eaeb8e15/build/mathbox-bundle'}\n",
    "});\n",
    "\n",
    "// Minified graphing functions\n",
    "\n",
    "window.with_mathbox=function(element,func){require(['mathBox'],function(){var mathbox=mathBox({plugins:['core','controls','cursor','mathbox'],controls:{klass:THREE.OrbitControls},mathbox:{inspect:!1},element:element[0],loop:{start:!1},});var three=mathbox.three;three.renderer.setClearColor(new THREE.Color(0xFFFFFF),1.0);three.camera.position.set(-1,1,2);three.controls.noKeys=!0;three.element.style.height=\"400px\";three.element.style.width=\"100%\";function isInViewport(element){var rect=element.getBoundingClientRect();var html=document.documentElement;var w=window.innerWidth||html.clientWidth;var h=window.innerHeight||html.clientHeight;return rect.top<h&&rect.left<w&&rect.bottom>0&&rect.right>0}\n",
    "var intervalId=setInterval(function(){if(three.element.offsetParent===null){clearInterval(intervalId);three.destroy();return}\n",
    "var visible=isInViewport(three.canvas);if(three.Loop.running!=visible){visible?three.Loop.start():three.Loop.stop()}},100);func(mathbox);window.dispatchEvent(new Event('resize'))})};window.plotGraph=function(mathbox,f,xlabel='x',ylabel='y',zlabel='f(x,y)',rng=[[-3,3],[-5,5],[-3,3]]){var view=mathbox.cartesian({range:rng,scale:[1,1,1]},{rotation:(t)=>[0,t*0.02,0]}).grid({axes:[1,3]})\n",
    "view.area({id:'yaxis',width:1,height:1,axes:[1,3],expr:function(emit,x,y,i,j){emit(4,0,0);emit(0,0,0)},items:2,channels:3,}).text({font:'Helvetica',style:'bold',width:16,height:5,depth:2,expr:function(emit,i,j,k,time){emit(ylabel)},}).label({color:'#000000',snap:!1,outline:2,size:24,offset:[0,-32],depth:.5,zIndex:1});view.vector({points:'#yaxis',color:0x000000,width:9,start:!0});view.area({id:'xaxis',width:1,height:1,axes:[1,3],expr:function(emit,x,y,i,j){emit(0,0,4);emit(0,0,0)},items:2,channels:3,}).text({font:'Helvetica',style:'bold',width:16,height:5,depth:2,expr:function(emit,i,j,k,time){emit(xlabel)},}).label({color:'#000000',snap:!1,outline:2,size:24,offset:[0,-32],depth:.5,zIndex:1,});view.vector({points:'#xaxis',color:0x000000,width:9,start:!0,});view.area({id:'zaxis',width:1,height:1,axes:[1,3],expr:function(emit,x,y,i,j){emit(0,4,0);emit(0,0,0)},items:2,channels:3,}).text({font:'Helvetica',style:'bold',width:16,height:5,depth:2,expr:function(emit,i,j,k,time){emit(zlabel)},}).label({color:'#000000',snap:!1,outline:2,size:24,offset:[0,-32],depth:.5,zIndex:1,});view.vector({points:'#zaxis',color:0x000000,width:9,start:!0,});var graph=view.area({id:'graph',width:64,height:64,axes:[1,3],expr:function(emit,y,x,i,j){emit(y,f(x,y),x)},items:1,channels:3,});view.surface({shaded:!0,lineX:!0,lineY:!0,points:graph,color:0x0000FF,width:1,});return view};window.addSegment=function(view,p0,p1,col){view.array({width:128,expr:function(emit,i,time){var b=i/128;var a=1-b;emit(a*p0[1]+b*p1[1],a*p0[2]+b*p1[2],a*p0[0]+b*p1[0])},channels:3,});view.line({color:col,width:10,size:2.5,stroke:'dotted',start:!1,end:!1,})};window.addPoint=function(view,p,col,label){view.array({width:4,items:2,channels:3,expr:function(emit,i,t){emit(p[1],p[2],p[0])},}).point({color:col,points:'<',size:15,depth:.5,zBias:50,}).text({font:'Helvetica',style:'bold',width:16,height:5,depth:2,expr:function(emit,i,j,k,time){emit(label)},}).label({color:col,snap:!1,outline:2,size:24,offset:[0,-32],depth:.5,zIndex:1,})};window.addCurve=function(view,ab,x,y,z,col){view.array({width:128,expr:function(emit,i,time){var t=(ab[1]-ab[0])*(i/128)+ab[0];emit(y(t),z(t),x(t))},channels:3,});view.line({color:col,width:20,size:2.5,start:!0,end:!0,})};window.addClosedCurve=function(view,ab,x,y,z,col){view.array({width:128,expr:function(emit,i,time){var t=(ab[1]-ab[0])*(i/128)+ab[0];emit(y(t),z(t),x(t))},channels:3,});view.line({color:col,width:20,size:2.5,start:!1,end:!1,})};window.addSurface=function(view,ab,cd,x,y,z,col,opa){view.matrix({width:64,height:64,expr:function(emit,i,j,time){var p=(ab[1]-ab[0])*(i/64)+ab[0];var q=(cd[1]-cd[0])*(j/64)+cd[0];emit(y(p,q),z(p,q),x(p,q))},items:1,channels:3}).surface({shaded:!0,lineX:!1,lineY:!1,color:col,width:1,opacity:opa})}\n",
    "window.addSequence=function(view,seq,col){var idx=0;var d=new Date();var start=d.getTime();view.array({width:1,expr:function(emit,i,time){var nd=new Date();var now=nd.getTime();if(1000<now-start){idx=idx+1;if(seq.length<=idx){idx=0}\n",
    "start=now}\n",
    "emit(seq[idx][1],seq[idx][2],seq[idx][0])},items:1,channels:3}).point({color:col,points:'<',size:15,depth:.5,zBias:50,})}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Important considerations\n",
    "\n",
    "Our concerns for bivariate optimization are the same as in the univariate case.\n",
    "\n",
    "1. Does $(P)$ have a minimum value?\n",
    "2. Does $(P)$ have a solution?\n",
    "3. Does $(P)$ have a **unique** solution?\n",
    "4. When $(P)$ has a minimum value, how can we find an $(\\widetilde{x},\\widetilde{y})$ such that $f(\\widetilde{x},\\widetilde{y})$ is close to the minimum value?\n",
    "5. When $(P)$ has a solution $(x^\\ast, y^\\ast)$, how can we find an $(\\widetilde{x},\\widetilde{y})$ which is close to $(x^\\ast, y^\\ast)$?\n",
    "\n",
    "We will explore how the answers to these questions differ from the univariate case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Existence of minimum values and minimizers\n",
    "\n",
    "If there is an $L\\in\\mathbb{R}$ such that $L\\leq f({\\bf x})$ for all ${\\bf x}\\in\\mathbb{R}^d$, then $f$ is said to be **bounded below**.\n",
    "\n",
    "#### Theorem: If $f:\\mathbb{R}^d\\rightarrow\\mathbb{R}$ is bounded below, then $(P)$ has a minimum value.\n",
    "\n",
    "Our next goal is to generalize the Extreme Value Theorem to general Euclidean spaces. For $\\varepsilon>0$, the **open $\\varepsilon$-ball** around ${\\bf x}\\in\\mathbb{R}^d$ is the set \n",
    "\n",
    "$$\n",
    "B({\\bf x},\\varepsilon)=\\{{\\bf y}\\in\\mathbb{R}^d: \\Vert {\\bf x}-{\\bf y}\\Vert<\\varepsilon\\}.\n",
    "$$\n",
    "\n",
    "Given $X\\subset\\mathbb{R}^d$, $f:X\\rightarrow\\mathbb{R}$ is said to be **continuous at** ${\\bf x}^{(0)}\\in\\mathbb{R}^d$ if for every $\\varepsilon>0$ there is a $\\delta>0$ such that $\\vert f({\\bf y})-f({\\bf x})\\vert<\\varepsilon$ for all ${\\bf y}\\in B({\\bf x},\\delta)\\cap X$. $f$ is **continuous** on $X$ (or just **continuous**) if $f$ is continuous at all ${\\bf x}\\in X$.\n",
    "\n",
    "For $X\\subset\\mathbb{R}^d$, a subset $U\\subset X$ is called **open** in $X$ if for every ${\\bf x}\\in U$ there is an $\\varepsilon>0$ such that $B({\\bf x},\\varepsilon)\\cap X\\subset U$. A subset $Q\\subset\\mathbb{R}^2$ is called **closed** in $X$ if its complement\n",
    "$$\n",
    "\\overline{Q}=\\{{\\bf x}\\in X: x\\not\\in Q\\}\n",
    "$$\n",
    "is open in $X$. The following theorem is very helpful for constrained optimization over $\\mathbb{R}^2$.\n",
    "\n",
    "#### Theorem: If $g_1, g_2,\\ldots, g_m:\\mathbb{R}^d\\rightarrow\\mathbb{R}$ and $h_1,h_2,\\ldots,h_n:\\mathbb{R}^d\\rightarrow\\mathbb{R}$ are all continuous functions, then the set $X=\\{{\\bf x}\\in\\mathbb{R}^d: g_i({\\bf x})=0, h_j({\\bf x})=0\\text{ for all }i,j\\}$ is closed.\n",
    "\n",
    "A set $X\\subset\\mathbb{R}^d$ is said to be **bounded** if there is an $R\\in\\mathbb{R}$ such that $X\\subset B({\\bf 0}, R)$.\n",
    "\n",
    "A set $X\\subset\\mathbb{R}$ is **compact** if it is both closed and bounded. \n",
    "\n",
    "#### Theorem (Extreme Value Theorem): If $X\\subset\\mathbb{R}^d$ is compact and $f:X\\rightarrow\\mathbb{R}$ is continuous on $X$, then $f$ has a minimizer ${\\bf x}^\\ast\\in X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV: Uniqueness of Solutions and Convexity\n",
    "Uniqueness of solutions is generally contingent upon convexity of the optimization program. \n",
    "\n",
    "A set $X\\subset\\mathbb{R}^d$ is said to be **convex** if for any ${\\bf x}, {\\bf y}\\in X$ and any $t\\in[0,1]$, $(1-t){\\bf x} + t{\\bf y}\\in X$. \n",
    "\n",
    "If $X\\subset\\mathbb{R}^2$ is a convex set:\n",
    "\n",
    "1. $f:X\\rightarrow\\mathbb{R}$ is said to be **convex** on $X$ if for every ${\\bf x}, {\\bf y}\\in X$ and every $t\\in[0,1]$ we have that\n",
    "$$\n",
    "f((1-t){\\bf x} + t{\\bf y}) \\leq (1-t)f({\\bf x}) + t f({\\bf y}).\n",
    "$$ \n",
    "2. $f:X\\rightarrow\\mathbb{R}$ is said to be **strictly convex** on $X$ if for every ${\\bf x}, {\\bf y}\\in X$ and every $t\\in(0,1)$ we have that\n",
    "$$\n",
    "f((1-t){\\bf x} + t{\\bf y}) < (1-t)f({\\bf x}) + t f({\\bf y}).\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again have that strict convexity implies convexity, which in turn implies continuity.\n",
    "\n",
    "#### Theorem (Convex Functions are Continuous): If $X\\subset\\mathbb{R}^d$ is convex and open, and $f:X\\rightarrow\\mathbb{R}$ is convex on $X$, then $f$ is continuous on $X$.\n",
    "\n",
    "A function $g:\\mathbb{R}^d\\rightarrow\\mathbb{R}^m$ is called **affine** if $g(t{\\bf x}+(1-t){\\bf y})=tg({\\bf x})+(1-t)g({\\bf y})$ for all $t\\in\\mathbb{R}$ and all ${\\bf x}, {\\bf y}\\in\\mathbb{R}^d$. It is easy to show that $g$ is affine if and only if there is an $A\\in M_{m, d}$ and a ${\\bf b}\\in \\mathbb{R}^m$ such that $g({\\bf x})=A{\\bf x} + {\\bf b}$. \n",
    "\n",
    "#### Theorem (Convex Domains): If $g:\\mathbb{R}^d\\rightarrow\\mathbb{R}^m$ is affine and $h_1,\\ldots, h_n:\\mathbb{R}^d\\rightarrow\\mathbb{R}$ are all convex functions, then the set $X=\\{{\\bf x}\\in\\mathbb{R}^d:g({\\bf x})={\\bf 0}, h_j({\\bf x})\\leq 0\\text{ for all }j\\}$ is convex.\n",
    "\n",
    "If $f$ is convex, then $(P)$ is called a **convex program**, and a constrained optimization program is called convex if the objective function is convex, the equality constraints are affine functions, and the inequality constraints are convex functions.\n",
    "\n",
    "Now, **strict minimizer**/**unique minimizer** of $f$ on $X$ is a point $(x^\\ast, y^\\ast)$ such that $f(x^\\ast,y^\\ast)<f(x,y)$ for all $(x,y)\\in X\\setminus\\{(x^\\ast,y^\\ast)\\}$. \n",
    "\n",
    "#### Theorem (Fundamental Theorem of Convex Programming): If $X\\subset\\mathbb{R}^d$ is convex, compact, and $f:X\\rightarrow\\mathbb{R}$ is  convex on $X$, then the set of minimizers of $f$ on $X$ form a convex set. Moreover, if $f$ strictly convex on $X$, then $f$ has a unique minimizer on $X$.\n",
    "\n",
    "Our first goal is to generalize the first order conditions for convexity. For convenience, we first define the **standard orthonormal basis** of $\\mathbb{R}^d$ as $\\{{\\bf e}^{(i)}\\}_{i=1}^d\\subset\\mathbb{R}^d$ where the $j$th entry of ${\\bf e}^{(i)}$ is given by\n",
    "\n",
    "$$\n",
    "{\\bf e}^{(i)}_j = \\left\\{\\begin{array}{cl}\n",
    "1 & \\text{ if }i=j\\\\\n",
    "0 & \\text{ if }i\\not=j\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "for all $i, j\\in \\{1, 2, \\ldots, d\\}$.\n",
    "\n",
    "We say that $f\\in C^1(\\mathbb{R}^d)$ if for each $i=1,\\ldots, d$ and each ${\\bf x}\\in\\mathbb{R}^d$,\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_i}({\\bf x}) = \\lim_{\\Delta x_i\\rightarrow 0} \\frac{f({\\bf x}+\\Delta x_i {\\bf e}^{(i)})-f({\\bf x})}{\\Delta x_i}\n",
    "$$\n",
    "is defined, and the functions $\\frac{\\partial f}{\\partial x_i}:\\mathbb{R}^d\\rightarrow\\mathbb{R}$ are all continuous.\n",
    "\n",
    "Now, generalizing from 2D, we have that the first order Taylor expansion of $f\\in C^1(\\mathbb{R}^d)$ at ${\\bf x}^{(0)}\\in\\mathbb{R}^d$ is $p_1({\\bf x})=f({\\bf x}^{(0)})+\\nabla f({\\bf x}^{(0)})^T({\\bf x}-{\\bf x}^{(0)})$, where the **gradient** of $f$ at ${\\bf x}^{(0)}$ is\n",
    "\n",
    "$$\n",
    "\\nabla f({\\bf x}^{(0)}) = \\begin{pmatrix}\n",
    "\\partial_1 f({\\bf x}^{(0)})\\\\\n",
    "\\partial_2 f({\\bf x}^{(0)})\\\\\n",
    "\\vdots\\\\\n",
    "\\partial_d f({\\bf x}^{(0)})\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1}({\\bf x}^{(0)})\\\\\n",
    "\\frac{\\partial f}{\\partial x_2}({\\bf x}^{(0)})\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\partial f}{\\partial x_d}({\\bf x}^{(0)})\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "The first order conditions for convexity in 2D are expressed compactly by $f({\\bf x})\\geq f({\\bf x}^{(0)})+\\nabla f({\\bf x}^{(0)})^T({\\bf x}-{\\bf x}^{(0)})$ for all ${\\bf x}, {\\bf x}^{(0)}$, and therefore the following theorem is a natural generalization.\n",
    "\n",
    "#### Theorem (First Order Conditions for Convexity): If $X\\subset\\mathbb{R}^d$ is convex, then $f\\in C^1(X)$ is convex if and only if $f({\\bf x})\\geq f({\\bf y}) + \\nabla f({\\bf y})^T({\\bf x}-{\\bf y})$ for all ${\\bf x}, {\\bf y}\\in X$. If $f({\\bf x})\\geq f({\\bf y}) + \\nabla f({\\bf y})^T({\\bf x}-{\\bf y})$ for all ${\\bf x}, {\\bf y}\\in X$ with ${\\bf x}\\not={\\bf y}$, then $f$ is strictly convex on $X$.\n",
    "\n",
    "We now generalize the second order conditions for convexity. We say that $f\\in C^2(\\mathbb{R}^d)$ if $f\\in C^1(\\mathbb{R}^d)$ and $\\frac{\\partial f}{\\partial x_i}\\in C^1(\\mathbb{R}^d)$ for each $i=1, 2,\\ldots, d$. The second order conditions require convexity for the second order Taylor approximations \n",
    "\n",
    "$$\n",
    "p_2({\\bf x}) = f({\\bf x}^{(0)}) + \\nabla f({\\bf x}^{(0)})^T({\\bf x}-{\\bf x}^{(0)}) + \\frac{1}{2}({\\bf x}-{\\bf x}^{(0)})^T\\nabla^2 f({\\bf x}^{(0)})({\\bf x}-{\\bf x}^{(0)}),\n",
    "$$\n",
    "\n",
    "for each ${\\bf x}^{(0)}\\in\\mathbb{R}^d$, and where the **Hessian** of $f$ at ${\\bf x}^{(0)}$ is \n",
    "\n",
    "$$\n",
    "\\nabla^2 f({\\bf x}^{(0)})= \\begin{pmatrix}\n",
    "\\partial_{1, 1} f({\\bf x}^{(0)}) & \\partial_{1, 2} f({\\bf x}^{(0)}) & \\cdots & \\partial_{1, d} f({\\bf x}^{(0)})\\\\\n",
    "\\partial_{1, 2} f({\\bf x}^{(0)}) & \\partial_{2, 2} f({\\bf x}^{(0)}) & \\cdots & \\partial_{2, d} f({\\bf x}^{(0)})\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\partial_{1, d} f({\\bf x}^{(0)}) & \\partial_{2, d} f({\\bf x}^{(0)}) & \\vdots & \\partial_{d, d} f({\\bf x}^{(0)})\n",
    "\\end{pmatrix}=\\begin{pmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial x_1\\partial x_1}({\\bf x}^{(0)}) & \\frac{\\partial^2 f}{\\partial x_1\\partial x_2}({\\bf x}^{(0)}) & \\cdots & \\frac{\\partial^2 f}{\\partial x_1\\partial x_d}({\\bf x}^{(0)})\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_1\\partial x_2}({\\bf x}^{(0)}) & \\frac{\\partial^2 f}{\\partial x_2\\partial x_2} f({\\bf x}^{(0)}) & \\cdots & \\frac{\\partial^2 f}{\\partial x_2\\partial x_d}({\\bf x}^{(0)})\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_1\\partial x_d} f({\\bf x}^{(0)}) & \\frac{\\partial^2 f}{\\partial x_2\\partial x_d}({\\bf x}^{(0)}) & \\vdots & \\frac{\\partial^2 f}{\\partial x_d\\partial x_d}({\\bf x}^{(0)})\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Now, the first order Taylor approximation to $p_2$ at ${\\bf y}\\in\\mathbb{R}^d$ is \n",
    "\n",
    "\\begin{eqnarray}\n",
    "q_1({\\bf x}) &=& f({\\bf x}^{(0)}) + \\nabla f({\\bf x}^{(0)})^T({\\bf y}-{\\bf x}^{(0)}) + \\frac{1}{2}({\\bf y}-{\\bf x}^{(0)})^T\\nabla^2 f({\\bf x}^{(0)})({\\bf y}-{\\bf x}^{(0)}) + \\left(\\nabla f({\\bf x}^{(0)}) + \\nabla^2 f({\\bf x}^{(0)})({\\bf y}-{\\bf x}^{(0)})\\right)^T({\\bf x}-{\\bf y})\\\\\n",
    "&=& f({\\bf x}^{(0)}) + \\nabla f({\\bf x}^{(0)})^T({\\bf x}-{\\bf x}^{(0)}) + \\frac{1}{2}({\\bf y}-{\\bf x}^{(0)})^T\\nabla^2 f({\\bf x}^{(0)})({\\bf y}-{\\bf x}^{(0)}) + ({\\bf y}-{\\bf x}^{(0)})^T\\nabla^2 f({\\bf x}^{(0)})({\\bf x}-{\\bf y})\\\\\n",
    "&=&f({\\bf x}^{(0)}) + \\nabla f({\\bf x}^{(0)})^T({\\bf x}-{\\bf x}^{(0)}) + \\frac{1}{2}({\\bf x}-{\\bf x}^{(0)})^T\\nabla^2 f({\\bf x}^{(0)})({\\bf x}-{\\bf x}^{(0)}) -\\frac{1}{2}({\\bf x}-{\\bf y})^T\\nabla^2 f({\\bf x}^{(0)})({\\bf x}-{\\bf y})\\\\\n",
    "&=& p_2({\\bf x}) -\\frac{1}{2}({\\bf x}-{\\bf y})^T\\nabla^2 f({\\bf x}^{(0)})({\\bf x}-{\\bf y})\n",
    "\\end{eqnarray}\n",
    "\n",
    "The first order conditions for convexity to $p_2$ are equivalent to $p_2({\\bf x})\\geq q_1({\\bf x})$ for all ${\\bf x},{\\bf y}\\in\\mathbb{R}^d$, and therefore we must have that\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} ({\\bf x}-{\\bf y})^T\\nabla^2 f({\\bf x}^{(0)})({\\bf x}-{\\bf y})\\geq 0\n",
    "$$\n",
    "\n",
    "for all ${\\bf x},{\\bf y}\\in\\mathbb{R}^d$. Setting ${\\bf z}= \\frac{1}{\\sqrt{2}}({\\bf x}-{\\bf y})$, and noting that ${\\bf x} = \\sqrt{2}{\\bf z}$, ${\\bf y}={\\bf 0}$ results in any particular ${\\bf z}\\in\\mathbb{R}^d$, we have that convexity of the second order Taylor approximation at ${\\bf x}^{(0)}$ is equivalent to \n",
    "\n",
    "$$\n",
    "{\\bf z}^T\\nabla^2 f({\\bf x}^{(0)}){\\bf z}\\geq 0\n",
    "$$\n",
    "\n",
    "for all ${\\bf z}\\in\\mathbb{R}^d$. Now, we let $M_{n,n}$ denote the $n$ by $n$ matrices with real entries, and if $A\\in M_{n,n}$ is **symmetric** ($A^T=A$), we say that $A$ is **positive semidefinite** if ${\\bf z}^T A{\\bf z}\\geq 0$ for all ${\\bf z}\\in\\mathbb{R}^d$. Thus, the second order conditions may be stated in terms of the positive definiteness of the Hessian $\\nabla^2 f({\\bf x}^{(0)})$ at every ${\\bf x}^{(0)}\\in\\mathbb{R}^d$:\n",
    "\n",
    "#### Theorem (Second Order Conditions for Convexity): If $X\\subset\\mathbb{R}^d$ is convex, then $f\\in C^2(X)$ is convex if and only if $\\nabla^2f({\\bf x}^{(0)})$ is positive semidefinite for all ${\\bf x}^{(0)}\\in X$. If $\\nabla^2f({\\bf x}^{(0)})$ is positive definite for all ${\\bf x}^{(0)}\\in X$, then $f$ is strictly convex. \n",
    "\n",
    "If $X\\subset\\mathbb{R}^d$ is convex, we say that $f\\in C^2(X)$ is **strongly convex** if there is a $c>0$ such that ${\\bf u}^T\\nabla^2 f({\\bf x}){\\bf u}\\geq c$ for all ${\\bf u}, {\\bf x}\\in\\mathbb{R}^d$ with $\\Vert u\\Vert=1$. \n",
    "\n",
    "Finally, we need some techniques to determine positive definiteness of a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive definite matrices\n",
    "\n",
    "We say that $U\\in M_{n, n}$ is **orthogonal** if $U^TU=I$, where $I$ is the $n$ by $n$ identity matrix (all diagonal entries are $1$'s and all other entries are $0$'s).\n",
    "\n",
    "#### Theorem : For a matrix $U\\in M_{n, n}$, the following are equivalent:\n",
    "1. $U$ is orthogonal\n",
    "2. the columns of $U$ form an orthonormal basis of $\\mathbb{R}^d$\n",
    "3. the rows of $U$ form an orthonormal basis of $\\mathbb{R}^d$\n",
    "4. $\\Vert U{\\bf x} \\Vert = \\Vert {\\bf x}\\Vert$ for all ${\\bf x}\\in\\mathbb{R}^d$\n",
    "\n",
    "For a vector ${\\bf v}\\in\\mathbb{R}$, we define\n",
    "\n",
    "$$\n",
    "\\text{diag}({\\bf v}) = \\begin{pmatrix}\n",
    "v_1 & 0 &\\cdots & 0 & 0\\\\\n",
    "0 & v_2 &\\cdots & 0 & 0\\\\\n",
    "\\vdots & \\vdots &\\ddots & \\vdots &\\vdots\\\\\n",
    "0 & 0 & \\cdots & v_{d-1} & 0\\\\\n",
    "0 & 0 & \\cdots & 0 & v_d\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "#### Theorem (Spectral Theorem): If $A\\in M_{n, n}$ is symmetric, then there exists ${\\bf v}\\in\\mathbb{R}^d$ and an orthogonal matrix $U\\in M_{n, n}$ such that $ A = U\\text{diag}({\\bf v}) U^T$. Moreover, the entries of ${\\bf v}$ list the eigenvalues of $A$ and the columns of $U$ are an orthonormal basis of eigenvectors for $A$.\n",
    "\n",
    "\n",
    "#### Theorem (Eigenvalue Characterization of Positive Definiteness): A symmetric matrix $A$ is positive semidefinite if and only if it has non-negative eigenvalues, and it is positive definite if and only if it has strictly postive eigenvalues.\n",
    "\n",
    "Computing the eigenvalues of $A$ is much more difficult when $d>2$. To establish an effective computational tool, we need a way to ensure positive definiteness. We will generalize Sylvester's criterion, but we must first introduce notation. \n",
    "\n",
    "We know the determinant of a $2$ by $2$ matrix, and the **Laplace expansion** of an $n$ by $n$ matrix $A$ is given by\n",
    "\n",
    "$$\n",
    "\\det\\begin{pmatrix}\n",
    "a_{1, 1} & a_{1, 2} & \\cdots & a_{1, n}\\\\\n",
    "a_{2, 1} & a_{2, 2} & \\cdots & a_{2, n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{n, 1} & a_{n, 2} & \\cdots & a_{n, n}\n",
    "\\end{pmatrix} = (-1)^{1+1}a_{1, 1} \\det\\begin{pmatrix}\n",
    "a_{2, 2} & a_{2, 3} & \\cdots & a_{2, n}\\\\\n",
    "a_{3, 2} & a_{3, 3} & \\cdots & a_{3, n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{n, 2} & a_{n, 3} & \\cdots & a_{n, n}\n",
    "\\end{pmatrix} + (-1)^{1+2} a_{1, 2} \\det\\begin{pmatrix}\n",
    "a_{2, 1} & a_{2, 3} & \\cdots & a_{2, n}\\\\\n",
    "a_{3, 1} & a_{3, 3} & \\cdots & a_{3, n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{n, 1} & a_{n, 3} & \\cdots & a_{n, n}\n",
    "\\end{pmatrix} +\\cdots + (-1)^{1+n} a_{1, n} \\det\\begin{pmatrix}\n",
    "a_{2, 1} & a_{2, 2} & \\cdots & a_{2, (n-1)}\\\\\n",
    "a_{3, 1} & a_{3, 2} & \\cdots & a_{3, (n-1)}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{n, 1} & a_{n, 2} & \\cdots & a_{n, (n-1)}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "That is, we expand along the *top row* of $A$ in a sum with alternating signs and terms which are the entry of the row times the determinant of the $(n-1)$ by $(n-1)$ submatrix of $A$ obtained by removing the first row and $j$th column of $A$. Now, if $A\\in M_{n, n}$ a **minor** of $A$ is any $k$ by $k$ submatrix of $A$ for $1\\leq k\\leq n$. In particular, we let $A_{i, j}$ denote the determinant of the $(n-1)$ by $(n-1)$ submatrix obtained by removing the $i$th row and $j$th column of $A$. Therefore the Laplace expansion may be written as \n",
    "\n",
    "$$\n",
    "\\det A = (-1)^{1+1} a_{1, 1} A_{1, 1} + (-1)^{1+2} a_{1, 2} A_{1, 2}+\\cdots + (-1)^{1+n}a_{1, n} A_{1, n}.\n",
    "$$\n",
    "\n",
    "It also makes sense to generalize the Laplace expansion to expansion along the $i$th row:\n",
    "\n",
    "$$\n",
    "\\det A = \\sum_{j=1}^d (-1)^{i+j} a_{i, j} A_{i, j}\n",
    "$$\n",
    "\n",
    "or, the $j$th column:\n",
    "\n",
    "$$\n",
    "\\det A = \\sum_{i=1}^d (-1)^{i+j} a_{i, j} A_{i, j}.\n",
    "$$\n",
    "\n",
    "Now, if a $k$ by $k$ minor of $A$ includes $k$ diagonal entries of $A$, the minor is called a **principal minor**. The $k$ by $k$ principal minor of $A$ formed from the first $k$ rows and columns of $A$ is called the a **leading principal minor**.\n",
    "\n",
    "### Example: \n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "2 & 1 & 0\\\\\n",
    "1 & 2 & 1\\\\\n",
    "0 & 1 & 2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "has $1$ by $1$ principal minors $2$, $2$; $2$, $2$ by $2$ principal minors\n",
    "$$\n",
    "\\det\\begin{pmatrix}\n",
    "2 & 1\\\\\n",
    "1 & 2\n",
    "\\end{pmatrix}=3, \\det\\begin{pmatrix}\n",
    "2 & 0\\\\\n",
    "0 & 2\n",
    "\\end{pmatrix}=4, \\text{ and } \\det\\begin{pmatrix}\n",
    "2 & 1\\\\\n",
    "1 & 2\n",
    "\\end{pmatrix}=4;\n",
    "$$\n",
    "and $3$ by $3$ principal minor\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "2 & 1 & 0\\\\\n",
    "1 & 2 & 1\\\\\n",
    "0 & 1 & 2\n",
    "\\end{pmatrix} = 2 (3) + (-1)(1)(2) + (0)(1)=4.\n",
    "$$\n",
    "The leading principal minors of $A$ are then $2$, $3$, and $4$.\n",
    "\n",
    "#### Theorem (Sylvester's Criterion): If $A\\in M_{n, n}$, then $A$ is positive definite if and only if the leading principal minors of $A$ are non-negative. Additionally, $A$ is positive semidefinite if and only if all of its principal minors are positive.\n",
    "\n",
    "Thus, to ensure convexity via the second order conditions, we compute the Hessian of $f$ at each ${\\bf x}\\in\\mathbb{R}^d$ and then check the Sylvester's criterion. This is still quite difficult because we have to check a matrix for positive definiteness for all ${\\bf x}\\in\\mathbb{R}^d$. Thus, we often rely upon other methods for determining convexity\n",
    "\n",
    "\n",
    "## Operations that preserve convexity\n",
    "\n",
    "#### Theorem (Positive Weighted Sum of Convex is Convex): If $X\\subset\\mathbb{R}^d$ is convex, $f,g:X\\rightarrow\\mathbb{R}$ are convex on $X$, and $a, b\\geq 0$, then $h:X\\rightarrow\\mathbb{R}$ defined by $h({\\bf x}) = af({\\bf x}) + bg({\\bf x})$ for all ${\\bf x}\\in X$ is convex on $X$.\n",
    "\n",
    "#### Theorem (Pointwise Maximum of Convex is Convex): If $X\\subset\\mathbb{R}^d$ is convex and $f, g:X\\rightarrow\\mathbb{R}$ are convex on $X$, then $h:X\\rightarrow\\mathbb{R}$ defined by $h({\\bf x}) = \\max(f({\\bf x}), g({\\bf x})$ for all ${\\bf x}$ is also convex on $X$.\n",
    "\n",
    "Recall that any function of the form $\\phi({\\bf x}) = A{\\bf x} + {\\bf b}$ for all ${\\bf x}\\in\\mathbb{R}^d$ where $A\\in M_{k, d}$ and ${\\bf b}\\in\\mathbb{R}^k$ is called **affine**.\n",
    "\n",
    "#### Theorem (Convexity Preservation under Affine Precomposition): Suppose $X\\subset\\mathbb{R}^k$ is convex, $f:X\\rightarrow\\mathbb{R}$ is convex on $X$, $A\\in M_{k, d}$ , ${\\bf b}\\in\\mathbb{R}^k$, and set $Y = \\{{\\bf y}\\in\\mathbb{R}^d: A{\\bf y}+{\\bf b}\\in X\\}$. Then $Y$ is convex and $g:Y\\rightarrow\\mathbb{R}$ defined by $g({\\bf y}) = f(A{\\bf y}+{\\bf b})$ for all ${\\bf y}\\in Y$ is convex on $Y$.\n",
    "\n",
    "#### Theorem (Convexity Preservation under Convex Monotone Transformation): Suppose $X\\subset\\mathbb{R}^d$ is convex, $f:X\\rightarrow\\mathbb{R}$ is convex on $X$, and that $g:f(X)\\rightarrow \\mathbb{R}$ is convex and non-decreasing, then $g\\circ f: X\\rightarrow\\mathbb{R}$ is convex on $X$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Questions\n",
    "\n",
    "1. If $f:\\mathbb{R}^2\\rightarrow\\mathbb{R}$ is convex on $\\mathbb{R}^2$, explain why the level sets $\\mathcal{L}(c) = \\{{\\bf x}\\in\\mathbb{R}^2: f({\\bf x}) \\leq c\\}$ are convex for any $c\\in\\mathbb{R}$.\n",
    "2. Compute the diagonalization of the matrix $\\displaystyle \\begin{pmatrix}2 & 1\\\\ 2 & 1\\end{pmatrix}$. \n",
    "3. Recall that $\\Vert {\\bf y}\\Vert\\geq 0$ for all ${\\bf y}\\in\\mathbb{R}^2$. If $A$ is an $m$ by $2$ matrix, explain why $A^T A$ is always positive semidefinite.\n",
    "4. Compute the second order Taylor approximation to $f(a, b) = 1/(1 + e^{2a + b})$. at $(a,b)=(1, 1)$.\n",
    "5. If the 2 by 2 matrix $A$ is symmetric and positive definite, explain why $A$ is necessarily invertible.\n",
    "6. Compute the second order Taylor approximation to $f(p, q) = p\\log p + q\\log q$ at $(p,q)=(1/2, 1/2)$.\n",
    "7. If the 2 by 2 matrix $A$ is symmetric, positive semidefinite, and invertible, explain why $A$ is necessarily positive definite.\n",
    "8. Compute the second order Taylor approximation to $f(a, b) = b^2/a$ at $(a, b) = (1, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4 Part II: Backpropagation\n",
    "\n",
    "## Block Jacobians and the Chain Rule\n",
    "\n",
    "Suppose $f\\in C^1\\left(\\mathcal{T}_{{\\bf n}_1}\\times\\mathcal{T}_{{\\bf n}_2};\\mathcal{T}_{{\\bf k}_1}\\right)$, $g\\in C^1\\left(\\mathcal{T}_{{\\bf m}_1}\\times\\mathcal{T}_{{\\bf m}_2};\\mathcal{T}_{{\\bf k}_2}\\right)$, and $h\\in C^1\\left(\\mathcal{T}_{{\\bf k}_1}\\times\\mathcal{T}_{{\\bf k}_2}, \\mathcal{T}_{{\\bf l}}\\right)$, then the function $\\varphi(\\mathcal{V}, \\mathcal{W},\\mathcal{X},\\mathcal{Y}) = h(f(\\mathcal{V},\\mathcal{W}),g(\\mathcal{X},\\mathcal{Y}))$ satisfies $\\varphi\\in C^1\\left(\\mathcal{T}_{{\\bf n}_1}\\times\\mathcal{T}_{{\\bf n}_2}\\times\\mathcal{T}_{{\\bf m}_1}\\times\\mathcal{T}_{{\\bf m}_2};\\mathcal{T}_{\\bf l}\\right)$. Because of the Cartesian product, we can't write down a single tensor for our Jacobian. Instead, we can get Jacobians for each block of variables:\n",
    "$$\n",
    "D_{\\mathcal{V}}\\varphi(\\mathcal{V}, \\mathcal{W},\\mathcal{X},\\mathcal{Y})\\in \\mathcal{T}_{{\\bf l}\\oplus{\\bf n}_1},\n",
    "$$\n",
    "$$\n",
    "D_{\\mathcal{W}}\\varphi(\\mathcal{V}, \\mathcal{W},\\mathcal{X},\\mathcal{Y})\\in \\mathcal{T}_{{\\bf l}\\oplus{\\bf n}_2},\n",
    "$$\n",
    "$$\n",
    "D_{\\mathcal{X}}\\varphi(\\mathcal{V}, \\mathcal{W},\\mathcal{X},\\mathcal{Y})\\in \\mathcal{T}_{{\\bf l}\\oplus{\\bf m}_1},\n",
    "$$\n",
    "and\n",
    "$$\n",
    "D_{\\mathcal{Y}}\\varphi(\\mathcal{V}, \\mathcal{W},\\mathcal{X},\\mathcal{Y})\\in \\mathcal{T}_{{\\bf l}\\oplus{\\bf m}_2},\n",
    "$$\n",
    "Thinking of $h$ as $h(\\mathcal{F},\\mathcal{G})$, we also have the Jacobian blocks\n",
    "$$\n",
    "D_{\\mathcal{F}} h(\\mathcal{F},\\mathcal{G})\\in \\mathcal{T}_{{\\bf l}\\oplus{\\bf k}_1},\\: D_{\\mathcal{G}} h(\\mathcal{F},\\mathcal{G})\\in \\mathcal{T}_{{\\bf l}\\oplus{\\bf k}_2},\n",
    "$$\n",
    "$$\n",
    "D_{\\mathcal{V}} f(\\mathcal{V},\\mathcal{W})\\in \\mathcal{T}_{{\\bf k}_1\\oplus{\\bf n}_1},\\: D_{\\mathcal{W}} f(\\mathcal{V},\\mathcal{W})\\in \\mathcal{T}_{{\\bf k}_1\\oplus{\\bf n}_2},\n",
    "$$\n",
    "$$\n",
    "D_{\\mathcal{X}} g(\\mathcal{X},\\mathcal{Y})\\in \\mathcal{T}_{{\\bf k}_2\\oplus{\\bf m}_1},\\: D_{\\mathcal{Y}} g(\\mathcal{X},\\mathcal{Y})\\in \\mathcal{T}_{{\\bf k}_2\\oplus{\\bf m}_2}\n",
    "$$\n",
    "The chain rule in this block formulation is then (suppressing arguments)\n",
    "$$\n",
    "D_{\\mathcal{V}}\\varphi_{({\\bf i},{\\bf j})} = c(D_{\\mathcal{F}}h, D_{\\mathcal{V}} f)_{({\\bf i},{\\bf j})}=\\left(\\sum_{\\bf k} \\frac{\\partial h_{\\bf i}}{\\partial f_{\\bf k}} \\frac{\\partial f_{\\bf k}}{\\partial v_{\\bf j}}\\right)_{({\\bf i},{\\bf j})}\n",
    "$$\n",
    "$$\n",
    "D_{\\mathcal{W}}\\varphi_{({\\bf i},{\\bf j})} = c(D_{\\mathcal{F}}h, D_{\\mathcal{W}} f)_{({\\bf i},{\\bf j})}=\\left(\\sum_{\\bf k} \\frac{\\partial h_{\\bf i}}{\\partial f_{\\bf k}} \\frac{\\partial f_{\\bf k}}{\\partial w_{\\bf j}}\\right)_{({\\bf i},{\\bf j})}\n",
    "$$\n",
    "$$\n",
    "D_{\\mathcal{X}}\\varphi_{({\\bf i},{\\bf j})} = c(D_{\\mathcal{G}}h, D_{\\mathcal{X}} g)_{({\\bf i},{\\bf j})}=\\left(\\sum_{\\bf k} \\frac{\\partial h_{\\bf i}}{\\partial g_{\\bf k}} \\frac{\\partial g_{\\bf k}}{\\partial x_{\\bf j}}\\right)_{({\\bf i},{\\bf j})}\n",
    "$$\n",
    "$$\n",
    "D_{\\mathcal{Y}}\\varphi_{({\\bf i},{\\bf j})} = c(D_{\\mathcal{G}}h, D_{\\mathcal{Y}} g)_{({\\bf i},{\\bf j})}=\\left(\\sum_{\\bf k} \\frac{\\partial h_{\\bf i}}{\\partial g_{\\bf k}} \\frac{\\partial g_{\\bf k}}{\\partial y_{\\bf j}}\\right)_{({\\bf i},{\\bf j})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "\n",
    "Consider a feedforward neural network with a single hidden layer. The objective function may be written as\n",
    "$$\n",
    "\\sum_{i=1}^N f_2({\\bf y}^{(i)}, f_1({\\bf x}^{(i)}; W, {\\bf b}); V, {\\bf c})\n",
    "$$\n",
    "for $W\\in \\mathcal{T}_{(k,d)}, {\\bf b}\\in \\mathcal{T}_{(k)}, V\\in\\mathcal{T}_{(k, m)}, {\\bf c}\\in\\mathcal{T}_{(m)}$. We will compute the gradient in blocks. First, we note that\n",
    "$$\n",
    "\\nabla_W\\sum_{i=1}^N f_2({\\bf y}^{(i)}, f_1({\\bf x}^{(i)}; W, {\\bf b}); V, {\\bf c})=\\sum_{i=1}^N \\nabla_W f_2({\\bf y}^{(i)}, f_1({\\bf x}^{(i)}; W, {\\bf b}); V, {\\bf c}).\n",
    "$$\n",
    "Moreover, the gradient is simply the transpose of the Jacobian, so we will simply compute\n",
    "$$\n",
    "D_W f_2({\\bf y}, f_1({\\bf x}; W, {\\bf b}); V, {\\bf c}).\n",
    "$$\n",
    "Viewing $f_2$ as $f_2({\\bf y},\\xi; V, {\\bf c})$, we then have that this Jacobian is the standard contraction of $D_\\xi f_2({\\bf y}, f_1({\\bf x}; W, {\\bf b}); V, {\\bf c})$ with $D_W f_1({\\bf x}; W, {\\bf b})$. Similarly, \n",
    "$$\n",
    "D_{\\bf b} f_2({\\bf y}, f_1({\\bf x}; W, {\\bf b}); V, {\\bf c})\n",
    "$$\n",
    "is the contraction of $D_\\xi f_2({\\bf y}, f_1({\\bf x}; W, {\\bf b}); V, {\\bf c})$ with $D_{\\bf b} f_1({\\bf x}; W, {\\bf b})$. Because $V$ and ${\\bf c}$ do not factor through compositions, their blocks are computable without the chain rule.\n",
    "\n",
    "This is a good start, but a single layer is generally determined by the composition of two functions. Let's consider a full composition of the form\n",
    "\n",
    "$$\n",
    "r(W_1, {\\bf b}^{(1)}, W_2, {\\bf b}^{(2)})=\\ell(Y; \\psi_2(\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)})))\n",
    "$$\n",
    "\n",
    "with \n",
    "\n",
    "\\begin{align}\n",
    "\\ell(Y; Q) &= -\\sum_{n=1}^N y_0^{(i)}\\log q_0^{(i)} + y_1^{(i)}\\log q_1^{(i)},\\\\\n",
    "\\psi_2(Z_2)&=\\text{softmax}(Z_2) \\text{ (applied to each row)},\\\\\n",
    "\\phi_2(X_2; W_2, {\\bf b}^{(2)}) &= X_2 W_2 +{\\bf b}^{(2)},\\\\\n",
    "\\psi_1(Z_1)&=\\text{logit}(Z_1)\\text{ (applied to each entry), and}\\\\\n",
    "\\phi_1(X_1; W_1, {\\bf b}^{(1)})&=X_1 W_1+{\\bf b}^{(1)}.\n",
    "\\end{align}\n",
    "\n",
    "And the chain rule gives us\n",
    "$$\\tiny\n",
    "D_{{\\bf b}^{(1)}} r = D_Q\\ell(Y;\\psi_2(\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)})))\\star D_{Z_2}\\psi_2(\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)}))\\star D_{X_2}\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)})\\star D_{Z_1}\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})) \\star D_{{\\bf b}^{(1)}} \\phi_1(X; W_1, {\\bf b}^{(1)})\n",
    "$$\n",
    "$$\\tiny\n",
    "D_{W_1} r = D_Q\\ell(Y;\\psi_2(\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)})))\\star D_{Z_2}\\psi_2(\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)}))\\star D_{X_2}\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)})\\star D_{Z_1}\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})) \\star D_{W_1} \\phi_1(X; W_1, {\\bf b}^{(1)})\n",
    "$$\n",
    "\n",
    "$$\\small\n",
    "D_{{\\bf b}^{(2)}} r = D_Q\\ell(Y;\\psi_2(\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)})))\\star D_{Z_2}\\psi_2(\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)}))\\star D_{{\\bf b}^{(2)}}\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)})\n",
    "$$\n",
    "\n",
    "$$\\small\n",
    "D_{W_2} r = D_Q\\ell(Y;\\psi_2(\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)})))\\star D_{Z_2}\\psi_2(\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)}))\\star D_{W_2}\\phi_2(\\psi_1(\\phi_1(X; W_1,{\\bf b}^{(1)})); W_2, {\\bf b}^{(2)})\n",
    "$$\n",
    "\n",
    "where we have used $\\star$ to denote contraction over indices associated with the variables of differentiation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/n8/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:38: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Avg Cross Entropy: 0.684925, Gradient Norm: 3.021409, Training Accuracy: 56.8 percent\n",
      "Step: 1, Avg Cross Entropy: 0.675549, Gradient Norm: 0.316272, Training Accuracy: 56.8 percent\n",
      "Step: 2, Avg Cross Entropy: 0.665785, Gradient Norm: 1.961342, Training Accuracy: 56.8 percent\n",
      "Step: 3, Avg Cross Entropy: 0.664687, Gradient Norm: 1.125810, Training Accuracy: 56.8 percent\n",
      "Step: 4, Avg Cross Entropy: 0.664309, Gradient Norm: 0.604068, Training Accuracy: 56.8 percent\n",
      "Step: 5, Avg Cross Entropy: 0.663998, Gradient Norm: 0.480739, Training Accuracy: 56.8 percent\n",
      "Step: 6, Avg Cross Entropy: 0.663598, Gradient Norm: 0.775358, Training Accuracy: 56.8 percent\n",
      "Step: 7, Avg Cross Entropy: 0.663291, Gradient Norm: 0.580687, Training Accuracy: 56.8 percent\n",
      "Step: 8, Avg Cross Entropy: 0.663083, Gradient Norm: 0.469624, Training Accuracy: 56.8 percent\n",
      "Step: 9, Avg Cross Entropy: 0.662664, Gradient Norm: 0.799686, Training Accuracy: 56.8 percent\n",
      "Step: 10, Avg Cross Entropy: 0.662368, Gradient Norm: 0.584834, Training Accuracy: 56.8 percent\n",
      "Step: 11, Avg Cross Entropy: 0.662230, Gradient Norm: 0.477273, Training Accuracy: 56.8 percent\n",
      "Step: 12, Avg Cross Entropy: 0.661829, Gradient Norm: 0.885567, Training Accuracy: 56.8 percent\n",
      "Step: 13, Avg Cross Entropy: 0.661506, Gradient Norm: 0.679527, Training Accuracy: 56.8 percent\n",
      "Step: 14, Avg Cross Entropy: 0.661233, Gradient Norm: 0.540975, Training Accuracy: 56.8 percent\n",
      "Step: 15, Avg Cross Entropy: 0.661063, Gradient Norm: 0.458573, Training Accuracy: 56.8 percent\n",
      "Step: 16, Avg Cross Entropy: 0.660698, Gradient Norm: 0.844443, Training Accuracy: 56.8 percent\n",
      "Step: 17, Avg Cross Entropy: 0.660390, Gradient Norm: 0.672866, Training Accuracy: 56.8 percent\n",
      "Step: 18, Avg Cross Entropy: 0.615676, Gradient Norm: 0.550979, Training Accuracy: 56.8 percent\n",
      "Step: 19, Avg Cross Entropy: 0.615434, Gradient Norm: 0.870377, Training Accuracy: 56.8 percent\n",
      "Step: 20, Avg Cross Entropy: 0.615364, Gradient Norm: 0.411037, Training Accuracy: 56.8 percent\n",
      "Step: 21, Avg Cross Entropy: 0.615221, Gradient Norm: 0.605631, Training Accuracy: 56.8 percent\n",
      "Step: 22, Avg Cross Entropy: 0.615119, Gradient Norm: 0.331021, Training Accuracy: 56.8 percent\n",
      "Step: 23, Avg Cross Entropy: 0.615070, Gradient Norm: 0.438396, Training Accuracy: 56.8 percent\n",
      "Step: 24, Avg Cross Entropy: 0.614886, Gradient Norm: 0.584559, Training Accuracy: 56.8 percent\n",
      "Step: 25, Avg Cross Entropy: 0.614569, Gradient Norm: 0.271976, Training Accuracy: 56.8 percent\n",
      "Step: 26, Avg Cross Entropy: 0.614287, Gradient Norm: 0.753314, Training Accuracy: 56.8 percent\n",
      "Step: 27, Avg Cross Entropy: 0.614065, Gradient Norm: 0.293088, Training Accuracy: 56.8 percent\n",
      "Step: 28, Avg Cross Entropy: 0.613621, Gradient Norm: 1.008426, Training Accuracy: 56.8 percent\n",
      "Step: 29, Avg Cross Entropy: 0.613492, Gradient Norm: 0.358224, Training Accuracy: 56.8 percent\n",
      "Step: 30, Avg Cross Entropy: 0.613161, Gradient Norm: 0.817114, Training Accuracy: 56.8 percent\n",
      "Step: 31, Avg Cross Entropy: 0.612811, Gradient Norm: 0.346977, Training Accuracy: 56.8 percent\n",
      "Step: 32, Avg Cross Entropy: 0.612205, Gradient Norm: 1.223272, Training Accuracy: 56.8 percent\n",
      "Step: 33, Avg Cross Entropy: 0.611978, Gradient Norm: 0.481062, Training Accuracy: 56.8 percent\n",
      "Step: 34, Avg Cross Entropy: 0.611757, Gradient Norm: 0.579044, Training Accuracy: 56.8 percent\n",
      "Step: 35, Avg Cross Entropy: 0.611476, Gradient Norm: 0.786115, Training Accuracy: 56.8 percent\n",
      "Step: 36, Avg Cross Entropy: 0.611215, Gradient Norm: 0.453460, Training Accuracy: 56.8 percent\n",
      "Step: 37, Avg Cross Entropy: 0.610770, Gradient Norm: 1.101224, Training Accuracy: 56.8 percent\n",
      "Step: 38, Avg Cross Entropy: 0.610521, Gradient Norm: 0.521305, Training Accuracy: 56.8 percent\n",
      "Step: 39, Avg Cross Entropy: 0.610384, Gradient Norm: 0.657404, Training Accuracy: 56.8 percent\n",
      "Step: 40, Avg Cross Entropy: 0.609996, Gradient Norm: 0.886652, Training Accuracy: 56.8 percent\n",
      "Step: 41, Avg Cross Entropy: 0.609701, Gradient Norm: 0.414086, Training Accuracy: 56.8 percent\n",
      "Step: 42, Avg Cross Entropy: 0.608902, Gradient Norm: 1.524219, Training Accuracy: 56.8 percent\n",
      "Step: 43, Avg Cross Entropy: 0.608787, Gradient Norm: 0.576029, Training Accuracy: 56.8 percent\n",
      "Step: 44, Avg Cross Entropy: 0.608499, Gradient Norm: 0.758701, Training Accuracy: 56.8 percent\n",
      "Step: 45, Avg Cross Entropy: 0.608092, Gradient Norm: 0.357284, Training Accuracy: 56.8 percent\n",
      "Step: 46, Avg Cross Entropy: 0.607559, Gradient Norm: 1.209889, Training Accuracy: 56.8 percent\n",
      "Step: 47, Avg Cross Entropy: 0.607390, Gradient Norm: 0.474595, Training Accuracy: 56.8 percent\n",
      "Step: 48, Avg Cross Entropy: 0.607258, Gradient Norm: 0.571739, Training Accuracy: 56.8 percent\n",
      "Step: 49, Avg Cross Entropy: 0.606995, Gradient Norm: 0.772878, Training Accuracy: 56.8 percent\n",
      "Step: 50, Avg Cross Entropy: 0.606765, Gradient Norm: 0.380989, Training Accuracy: 56.8 percent\n",
      "Step: 51, Avg Cross Entropy: 0.606466, Gradient Norm: 0.846712, Training Accuracy: 56.8 percent\n",
      "Step: 52, Avg Cross Entropy: 0.606294, Gradient Norm: 0.393597, Training Accuracy: 56.8 percent\n",
      "Step: 53, Avg Cross Entropy: 0.605941, Gradient Norm: 0.947666, Training Accuracy: 56.8 percent\n",
      "Step: 54, Avg Cross Entropy: 0.605869, Gradient Norm: 0.414395, Training Accuracy: 56.8 percent\n",
      "Step: 55, Avg Cross Entropy: 0.605422, Gradient Norm: 1.100736, Training Accuracy: 56.8 percent\n",
      "Step: 56, Avg Cross Entropy: 0.605241, Gradient Norm: 0.451705, Training Accuracy: 56.8 percent\n",
      "Step: 57, Avg Cross Entropy: 0.605102, Gradient Norm: 0.537287, Training Accuracy: 56.8 percent\n",
      "Step: 58, Avg Cross Entropy: 0.604862, Gradient Norm: 0.708702, Training Accuracy: 56.8 percent\n",
      "Step: 59, Avg Cross Entropy: 0.604757, Gradient Norm: 0.361883, Training Accuracy: 56.8 percent\n",
      "Step: 60, Avg Cross Entropy: 0.604011, Gradient Norm: 1.540058, Training Accuracy: 56.8 percent\n",
      "Step: 61, Avg Cross Entropy: 0.603800, Gradient Norm: 0.626815, Training Accuracy: 56.8 percent\n",
      "Step: 62, Avg Cross Entropy: 0.603306, Gradient Norm: 0.346449, Training Accuracy: 80.2 percent\n",
      "Step: 63, Avg Cross Entropy: 0.602873, Gradient Norm: 1.052502, Training Accuracy: 82.5 percent\n",
      "Step: 64, Avg Cross Entropy: 0.602694, Gradient Norm: 0.431436, Training Accuracy: 82.5 percent\n",
      "Step: 65, Avg Cross Entropy: 0.602544, Gradient Norm: 0.508255, Training Accuracy: 84.2 percent\n",
      "Step: 66, Avg Cross Entropy: 0.602329, Gradient Norm: 0.681874, Training Accuracy: 84.2 percent\n",
      "Step: 67, Avg Cross Entropy: 0.602097, Gradient Norm: 0.373634, Training Accuracy: 86.8 percent\n",
      "Step: 68, Avg Cross Entropy: 0.601820, Gradient Norm: 0.853531, Training Accuracy: 85.2 percent\n",
      "Step: 69, Avg Cross Entropy: 0.601637, Gradient Norm: 0.423879, Training Accuracy: 86.8 percent\n",
      "Step: 70, Avg Cross Entropy: 0.601513, Gradient Norm: 0.519172, Training Accuracy: 85.5 percent\n",
      "Step: 71, Avg Cross Entropy: 0.601270, Gradient Norm: 0.685951, Training Accuracy: 86.8 percent\n",
      "Step: 72, Avg Cross Entropy: 0.600915, Gradient Norm: 0.351556, Training Accuracy: 85.5 percent\n",
      "Step: 73, Avg Cross Entropy: 0.600361, Gradient Norm: 1.210819, Training Accuracy: 87.5 percent\n",
      "Step: 74, Avg Cross Entropy: 0.600224, Gradient Norm: 0.482723, Training Accuracy: 86.5 percent\n",
      "Step: 75, Avg Cross Entropy: 0.600007, Gradient Norm: 0.627770, Training Accuracy: 87.5 percent\n",
      "Step: 76, Avg Cross Entropy: 0.599551, Gradient Norm: 0.346235, Training Accuracy: 86.5 percent\n",
      "Step: 77, Avg Cross Entropy: 0.599089, Gradient Norm: 1.068814, Training Accuracy: 88.2 percent\n",
      "Step: 78, Avg Cross Entropy: 0.598938, Gradient Norm: 0.442753, Training Accuracy: 87.2 percent\n",
      "Step: 79, Avg Cross Entropy: 0.598852, Gradient Norm: 0.566396, Training Accuracy: 89.0 percent\n",
      "Step: 80, Avg Cross Entropy: 0.598588, Gradient Norm: 0.892805, Training Accuracy: 87.5 percent\n",
      "Step: 81, Avg Cross Entropy: 0.598453, Gradient Norm: 0.481788, Training Accuracy: 89.0 percent\n",
      "Step: 82, Avg Cross Entropy: 0.598253, Gradient Norm: 0.713724, Training Accuracy: 87.8 percent\n",
      "Step: 83, Avg Cross Entropy: 0.598090, Gradient Norm: 0.424345, Training Accuracy: 89.0 percent\n",
      "Step: 84, Avg Cross Entropy: 0.597928, Gradient Norm: 0.585956, Training Accuracy: 88.5 percent\n",
      "Step: 85, Avg Cross Entropy: 0.597829, Gradient Norm: 0.387165, Training Accuracy: 89.0 percent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 86, Avg Cross Entropy: 0.597480, Gradient Norm: 1.098624, Training Accuracy: 87.8 percent\n",
      "Step: 87, Avg Cross Entropy: 0.597292, Gradient Norm: 0.573762, Training Accuracy: 89.0 percent\n",
      "Step: 88, Avg Cross Entropy: 0.597004, Gradient Norm: 0.351290, Training Accuracy: 87.2 percent\n",
      "Step: 89, Avg Cross Entropy: 0.596403, Gradient Norm: 1.254805, Training Accuracy: 89.2 percent\n",
      "Step: 90, Avg Cross Entropy: 0.596343, Gradient Norm: 0.529551, Training Accuracy: 87.8 percent\n",
      "Step: 91, Avg Cross Entropy: 0.596053, Gradient Norm: 0.780033, Training Accuracy: 89.2 percent\n",
      "Step: 92, Avg Cross Entropy: 0.595954, Gradient Norm: 0.391757, Training Accuracy: 87.8 percent\n",
      "Step: 93, Avg Cross Entropy: 0.595549, Gradient Norm: 0.975885, Training Accuracy: 89.2 percent\n",
      "Step: 94, Avg Cross Entropy: 0.595424, Gradient Norm: 0.445161, Training Accuracy: 88.8 percent\n",
      "Step: 95, Avg Cross Entropy: 0.595221, Gradient Norm: 0.624149, Training Accuracy: 89.2 percent\n",
      "Step: 96, Avg Cross Entropy: 0.595047, Gradient Norm: 0.375160, Training Accuracy: 88.8 percent\n",
      "Step: 97, Avg Cross Entropy: 0.594716, Gradient Norm: 0.861971, Training Accuracy: 89.5 percent\n",
      "Step: 98, Avg Cross Entropy: 0.594582, Gradient Norm: 0.426665, Training Accuracy: 89.0 percent\n",
      "Step: 99, Avg Cross Entropy: 0.594395, Gradient Norm: 0.598559, Training Accuracy: 89.5 percent\n",
      "Final test accuracy: 93.5 percent\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "def chain_rule(Dg, Df, var_shape):\n",
    "    # Computes the Jacobian D (g o f)\n",
    "    dim = len(var_shape)\n",
    "    Dg_axes = list(range(Dg.ndim-dim, Dg.ndim))\n",
    "    Df_axes = list(range(dim))\n",
    "    return np.tensordot(Dg, Df, axes=(Dg_axes, Df_axes))\n",
    "\n",
    "# Compute the Jacobian blocks of X @ W + b\n",
    "\n",
    "def DX_affine(X, W, b):\n",
    "    # (d_{x_{i, j}} (X @ W))_{a, b} = e_a^T e_ie_j^T W e_b, so a,i slices equal W.T\n",
    "    D = np.zeros((X.shape[0], W.shape[1], X.shape[0], X.shape[1]))\n",
    "    for k in range(X.shape[0]):\n",
    "        D[k,:,k,:]=W.T\n",
    "    return D, X.shape\n",
    "\n",
    "def DW_affine(X, W, b):\n",
    "    # (d_{w_{i, j}} (X @ W))_{a, b} = e_a^T X e_ie_j^T e_b, so b, j slices equal x\n",
    "    D = np.zeros((X.shape[0], W.shape[1], W.shape[0], W.shape[1]))\n",
    "    for k in range(W.shape[1]):\n",
    "        D[:,k,:,k]=X\n",
    "    return D, W.shape\n",
    "\n",
    "def Db_affine(X, W, b):\n",
    "    # (d_{b_i} (1 @ b))_{a, b} = e_a^T 1 e_i^T e_b, so b, i slices are all ones\n",
    "    D = np.zeros((X.shape[0], W.shape[1], b.shape[1]))\n",
    "    for k in range(b.shape[1]):\n",
    "        D[:,k,k]=1\n",
    "    return D, b.shape\n",
    "    \n",
    "def logit(z):\n",
    "    # This is vectorized\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def Dlogit(Z):\n",
    "    # The Jacobian of the matrix logit\n",
    "    D = np.zeros((Z.shape[0], Z.shape[1], Z.shape[0], Z.shape[1]))\n",
    "    A = logit(Z) * logit(-Z)\n",
    "    for i in range(Z.shape[0]):\n",
    "        for j in range(Z.shape[1]):\n",
    "            D[i,j,i,j] = A[i,j]\n",
    "    return D, Z.shape\n",
    "\n",
    "def softmax(z):\n",
    "    v = np.exp(z)\n",
    "    return v / np.sum(v)\n",
    "\n",
    "def matrix_softmax(Z):\n",
    "    return np.apply_along_axis(softmax, 1, Z)\n",
    "\n",
    "def Dmatrix_softmax(Z):\n",
    "    D = np.zeros((Z.shape[0], Z.shape[1], Z.shape[0], Z.shape[1]))\n",
    "    for k in range(Z.shape[0]):\n",
    "        v = np.exp(Z[k,:])\n",
    "        v = v / np.sum(v)\n",
    "        D[k,:,k,:] = np.diag(v) - np.outer(v,v)\n",
    "        #print(D[k,:,k,:])\n",
    "    return D, Z.shape\n",
    "\n",
    "def cross_entropy(P, Q):\n",
    "    return -np.sum(P * np.log(Q))/P.shape[0]\n",
    "\n",
    "def DQcross_entropy(P, Q):\n",
    "    return - P * (1/Q)/P.shape[0], Q.shape\n",
    "\n",
    "def nn_loss_closure(X, Y):\n",
    "    # vars[0]=W_1, vars[1]=b_1, vars[2]=W_2, vars[3]=b_2\n",
    "    # cross_entropy(Y, matrix_softmax(affine(logit(affine(X; W_1, b_1))); W_2, b_2))\n",
    "    def f(var):\n",
    "        return cross_entropy(Y, matrix_softmax((logit((X @ var[0]) + var[1]) @ var[2]) + var[3]))\n",
    "    return f\n",
    "\n",
    "def nn_loss_gradient_closure(X, Y):\n",
    "    def df(var):\n",
    "        # Activation of first layer\n",
    "        Z1 = (X @ var[0]) + var[1]\n",
    "        X2 = logit(Z1)\n",
    "        \n",
    "        # Activation of second layer\n",
    "        Z2 = (X2 @ var[2]) + var[3]\n",
    "        Q = matrix_softmax(Z2)\n",
    "        \n",
    "        # Backpropagation tells us we can immediately contract DQ DZ2\n",
    "        D_Q, Qshape = DQcross_entropy(Y, Q)\n",
    "        D_Z2, Z2shape = Dmatrix_softmax(Z2)\n",
    "        back_prop2 = chain_rule(D_Q, D_Z2, Qshape)\n",
    "        \n",
    "        # Jacobians for phi_2\n",
    "        D_X2, X2shape = DX_affine(X2, var[2], var[3])\n",
    "        D_W2, W2shape = DW_affine(X2, var[2], var[3])\n",
    "        D_b2, b2shape = Db_affine(X2, var[2], var[3])\n",
    "        \n",
    "        # Jacobian for psi_1\n",
    "        D_Z1, Z1shape = Dlogit(Z1)\n",
    "        back_prop1 = chain_rule(chain_rule(back_prop2, D_X2, X2shape), D_Z1, Z1shape)\n",
    "        \n",
    "        # Jacobians for phi_1\n",
    "        D_W1, W1shape = DW_affine(X, var[0], var[1])\n",
    "        D_b1, b1shape = Db_affine(X, var[0], var[1])\n",
    "        \n",
    "        # Compute all the gradients\n",
    "        W1grad = chain_rule(back_prop1, D_W1, W1shape)\n",
    "        b1grad = chain_rule(back_prop1, D_b1, b1shape)\n",
    "        W2grad = chain_rule(back_prop2, D_W2, W2shape)\n",
    "        b2grad = chain_rule(back_prop2, D_b2, b2shape)\n",
    "        \n",
    "        return [W1grad, b1grad, W2grad, b2grad]\n",
    "    return df\n",
    "\n",
    "def update_blocks(x,y,t):\n",
    "    # An auxiliary function for backtracking with blocks of variables\n",
    "    num_blocks = len(x)\n",
    "    z = [None]*num_blocks\n",
    "    for i in range(num_blocks):\n",
    "        z[i] = x[i] + t*y[i]\n",
    "    return z\n",
    "                           \n",
    "def block_backtracking(x0, f, dx, df0, alpha=0.1, beta=0.5, verbose=False):\n",
    "    num_blocks = len(x0)\n",
    "    \n",
    "    delta = 0\n",
    "    for i in range(num_blocks):\n",
    "        delta = delta + np.sum(dx[i] * df0[i])\n",
    "    delta = alpha * delta\n",
    "    \n",
    "    f0 = f(x0)\n",
    "    \n",
    "    t = 1\n",
    "    x = update_blocks(x0, dx, t)\n",
    "    fx = f(x)\n",
    "    while (not np.isfinite(fx)) or f0+t*delta<fx:\n",
    "        t = beta*t\n",
    "        x = update_blocks(x0, dx, t)\n",
    "        fx = f(x)\n",
    "        \n",
    "    if verbose:\n",
    "        print((t, delta))\n",
    "        l=-1e-5\n",
    "        u=1e-5\n",
    "        s = np.linspace(l, u, 64)\n",
    "        fs = np.zeros(s.size)\n",
    "        crit = f0 + s*delta\n",
    "        tan = f0 + s*delta/alpha\n",
    "        for i in range(s.size):\n",
    "            fs[i] = f(update_blocks(x0, dx, s[i]))\n",
    "        plt.plot(s, fs)\n",
    "        plt.plot(s, crit, '--')\n",
    "        plt.plot(s, tan, '.')\n",
    "        plt.scatter([0], [f0])\n",
    "        plt.show()\n",
    "            \n",
    "    return x, fx\n",
    "\n",
    "def negate_blocks(x):\n",
    "    # Helper function for negating the gradient of block variables\n",
    "    num_blocks = len(x)\n",
    "    z = [None]*num_blocks\n",
    "    for i in range(num_blocks):\n",
    "        z[i] = -x[i]\n",
    "    return z\n",
    "\n",
    "def block_norm(x):\n",
    "    num_blocks=len(x)\n",
    "    z = 0\n",
    "    for i in range(num_blocks):\n",
    "        z = z + np.sum(x[i]**2)\n",
    "    return np.sqrt(z)\n",
    "\n",
    "def random_matrix(shape, sigma=0.1):\n",
    "    # Helper for random initialization\n",
    "    return np.reshape(sigma*rd.randn(shape[0]*shape[1]), shape)\n",
    "\n",
    "### Begin gradient descent\n",
    "\n",
    "data = load_breast_cancer() # Loads the Wisconsin Breast Cancer dataset (569 examples in 30 dimensions)\n",
    "\n",
    "# Parameters for the data\n",
    "dim_data = 30\n",
    "num_labels = 2\n",
    "num_examples = 569\n",
    "\n",
    "# Parameters for training\n",
    "num_train = 400\n",
    "\n",
    "X = data['data'] # Data in rows\n",
    "targets = data.target # 0-1 labels\n",
    "labels = np.zeros((num_examples, num_labels))\n",
    "for i in range(num_examples):\n",
    "    labels[i,targets[i]]=1 # Conversion to one-hot representations\n",
    "\n",
    "# Prepare hyperparameters of the network\n",
    "hidden_nodes = 20\n",
    "\n",
    "# Initialize variables\n",
    "W1_init = random_matrix((dim_data, hidden_nodes))\n",
    "b1_init = np.zeros((1, hidden_nodes))\n",
    "\n",
    "W2_init = random_matrix((hidden_nodes, num_labels))\n",
    "b2_init = np.zeros((1, num_labels))\n",
    "\n",
    "x = [W1_init, b1_init, W2_init, b2_init]\n",
    "f = nn_loss_closure(X[:num_train,:], labels[:num_train,:])\n",
    "df = nn_loss_gradient_closure(X[:num_train,:], labels[:num_train,:])\n",
    "dx = lambda v: negate_blocks(df(v))\n",
    "    \n",
    "for i in range(100):\n",
    "    ngrad = dx(x)\n",
    "    x, fval = block_backtracking(x, f, ngrad, df(x), alpha=0.1, verbose=False)\n",
    "    \n",
    "    train_data = matrix_softmax(logit(X[:num_train,:]@x[0] + x[1]) @ x[2] + x[3])\n",
    "    train_labels = np.argmax(train_data, axis=1)\n",
    "    per_correct = 100*(1 - np.count_nonzero(train_labels - targets[:num_train])/num_train)\n",
    "\n",
    "    print(\"Step: %d, Avg Cross Entropy: %f, Gradient Norm: %f, Training Accuracy: %.1f percent\" % (i,fval,block_norm(ngrad), per_correct))\n",
    "    \n",
    "test_data = matrix_softmax(logit(X[num_train:,:]@x[0] + x[1]) @ x[2] + x[3])\n",
    "test_labels = np.argmax(test_data, axis=1)\n",
    "per_correct = 100*(1 - np.count_nonzero(test_labels - targets[num_train:])/(num_examples-num_train))\n",
    "\n",
    "print('Final test accuracy: %.1f percent' % per_correct)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Consider the functions $f_1, f_2, f_3:\\mathbb{R}^2\\rightarrow\\mathbb{R}$ as $f_1(x_1,\\theta_1)$, $f_2(x_2,\\theta_2)$, and $f_3(x_3,\\theta_3)$. Then define\n",
    "\n",
    "$$\n",
    "g(x; \\theta_1, \\theta_2, \\theta_3) = f_3(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3).\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\frac{\\partial g}{\\partial\\theta_1}(x;\\theta_1, \\theta_2, \\theta_3)=\\frac{\\partial f_3}{\\partial x_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3)\\frac{\\partial}{\\partial\\theta_1}\\left[f_2(f_1(x,\\theta_1),\\theta_2)\\right] + \\frac{\\partial f_3}{\\partial \\theta_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3)\\frac{\\partial \\theta_3}{\\partial\\theta_1}=\\frac{\\partial f_3}{\\partial x_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3)\\left[\\frac{\\partial f_2}{\\partial x_2}(f_1(x,\\theta_1),\\theta_2)\\frac{\\partial f_1}{\\partial\\theta_1}(x,\\theta_1) + \\frac{\\partial f_2}{\\partial \\theta_2}(f_1(x,\\theta_1),\\theta_2)\\frac{\\partial \\theta_2}{\\partial \\theta_1}\\right] = \\frac{\\partial f_3}{\\partial x_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3)\\frac{\\partial f_2}{\\partial x_2}(f_1(x,\\theta_1),\\theta_2)\\frac{\\partial f_1}{\\partial\\theta_1}(x,\\theta_1).\n",
    "$$\n",
    "\n",
    "Similarly,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial g}{\\partial\\theta_2}(x;\\theta_1, \\theta_2, \\theta_3) = \\frac{\\partial f_3}{\\partial x_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3)\\frac{\\partial f_2}{\\partial \\theta_2}(f_1(x,\\theta_1),\\theta_2)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{\\partial g}{\\partial\\theta_3}(x;\\theta_1, \\theta_2, \\theta_3) = \\frac{\\partial f_3}{\\partial \\theta_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3).\n",
    "$$\n",
    "\n",
    "Another way to see this is to view this as the sequence of maps\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\theta_1\\\\\n",
    "\\theta_2\\\\\n",
    "\\theta_3\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_1(x,\\theta_1)\\\\\n",
    "\\theta_2\\\\\n",
    "\\theta_3\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_2(f_1(x,\\theta_1),\\theta_2)\\\\\n",
    "\\theta_3\n",
    "\\end{pmatrix}\\longmapsto f_3(f_2(f_1(x,\\theta_1),\\theta_2)\\theta_3).\n",
    "$$\n",
    "\n",
    "The Jacobians of these maps are\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial f_1}{\\partial\\theta_1}(x,\\theta_1) & 0 & 0\\\\\n",
    "0 & 1 & 0\\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}, \\begin{pmatrix}\n",
    "\\frac{\\partial f_2}{\\partial x_2}(f_1(x,\\theta_1),\\theta_2) & \\frac{\\partial f_2}{\\partial \\theta_2}(f_1(x,\\theta_1),\\theta_2) & 0\\\\\n",
    "0 & 0 & 1\\\\\n",
    "\\end{pmatrix},\\text{ and } \\begin{pmatrix}\n",
    "\\frac{\\partial f_3}{\\partial x_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3) & \\frac{\\partial f_3}{\\partial \\theta_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The interesting thing to note here is that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_3}{\\partial x_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3)\n",
    "$$\n",
    "\n",
    "is a factor for two of these partial derivatives. This redundancy is more pronounced as we get a deeper composition. Consider the composition of maps\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\theta_1\\\\\n",
    "\\theta_2\\\\\n",
    "\\theta_3\\\\\n",
    "\\theta_4\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_1(x,\\theta_1)\\\\\n",
    "\\theta_2\\\\\n",
    "\\theta_3\\\\\n",
    "\\theta_4\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_2(f_1(x,\\theta_1),\\theta_2)\\\\\n",
    "\\theta_3\\\\\n",
    "\\theta_4\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_3(f_2(f_1(x,\\theta_1),\\theta_2)\\theta_3)\\\\\n",
    "\\theta_4\n",
    "\\end{pmatrix}\\longmapsto f_4(f_3(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3),\\theta_4).\n",
    "$$\n",
    "\n",
    "The Jacobian of this composition (by the chain rule)\n",
    "\n",
    "$$\n",
    "\\tiny\\begin{pmatrix}\n",
    "\\frac{\\partial f_4}{\\partial x_4}(f_3(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3), \\theta_4) & \\frac{\\partial f_4}{\\partial \\theta_4}(f_3(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3), \\theta_4)\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "\\frac{\\partial f_3}{\\partial x_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3) & \\frac{\\partial f_3}{\\partial \\theta_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3) & 0\\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "\\frac{\\partial f_2}{\\partial x_2}(f_1(x,\\theta_1),\\theta_2) & \\frac{\\partial f_2}{\\partial \\theta_2}(f_1(x,\\theta_1),\\theta_2) & 0 & 0\\\\\n",
    "0 & 0 & 1 & 0\\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "\\frac{\\partial f_1}{\\partial\\theta_1}(x,\\theta_1) & 0 & 0 & 0\\\\\n",
    "0 & 1 & 0 & 0\\\\\n",
    "0 & 0 & 1 & 0\\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{pmatrix}, \n",
    "$$\n",
    "\n",
    "This means that the gradient has the form (with suppression of arguments):\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial f_4}{\\partial x_4} \\frac{\\partial f_3}{\\partial x_3} \\frac{\\partial f_2}{\\partial x_2} \\frac{\\partial f_1}{\\partial \\theta_1}\\\\\n",
    "\\frac{\\partial f_4}{\\partial x_4} \\frac{\\partial f_3}{\\partial x_3} \\frac{\\partial f_2}{\\partial \\theta_2}\\\\\n",
    "\\frac{\\partial f_4}{\\partial x_4} \\frac{\\partial f_3}{\\partial \\theta_3}\\\\\n",
    "\\frac{\\partial f_4}{\\partial \\theta_4}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "This suggests the following computational structure for computing a gradient descent update using step size $\\eta>0$:\n",
    "\n",
    "1. $\\theta_4^\\prime = \\theta_4 - \\eta \\frac{\\partial f_4}{\\partial \\theta_4}$ and set $q=\\frac{\\partial f_4}{\\partial x_4}$.\n",
    "2. For $i=3, 2, 1$: set $\\theta_i^\\prime = \\theta_i -\\eta q \\frac{\\partial f_i}{\\partial \\theta_i}$ and $q= q \\frac{\\partial f_i}{\\partial x_i}$\n",
    "\n",
    "This is a simplified version of the **backpropagation** algorithm (or, backprop).  Now, let's suppose that $f_1({\\bf x}_1, \\Theta_1)$, $f_2({\\bf x}_2, \\Theta_2)$, $f_3({\\bf x}_3,\\Theta_3)$, and $f_4({\\bf x}_4, \\Theta_4)$ where the ${\\bf x}$'s and $\\Theta$'s are vectors of parameters. Then our composition has a *block form* given by\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\Theta_1\\\\\n",
    "\\Theta_2\\\\\n",
    "\\Theta_3\\\\\n",
    "\\Theta_4\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_1({\\bf x},\\Theta_1)\\\\\n",
    "\\Theta_2\\\\\n",
    "\\Theta_3\\\\\n",
    "\\Theta_4\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_2(f_1({\\bf x},\\Theta_1),\\Theta_2)\\\\\n",
    "\\Theta_3\\\\\n",
    "\\Theta_4\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_3(f_2(f_1({\\bf x},\\Theta_1),\\Theta_2)\\Theta_3)\\\\\n",
    "\\Theta_4\n",
    "\\end{pmatrix}\\longmapsto f_4(f_3(f_2(f_1({\\bf x},\\Theta_1),\\Theta_2),\\Theta_3),\\Theta_4).\n",
    "$$\n",
    "\n",
    "and the chain rule gives the Jacobian (in block form)\n",
    "\n",
    "$$\n",
    "\\tiny\\begin{pmatrix}\n",
    "D_{{\\bf x}_4}f_4(f_3(f_2(f_1({\\bf x},\\Theta_1),\\Theta_2),\\Theta_3), \\Theta_4) & D_{\\Theta_4}f_4(f_3(f_2(f_1({\\bf x},\\Theta_1),\\Theta_2),\\Theta_3), \\Theta_4)\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "D_{{\\bf x}_3} f_3(f_2(f_1({\\bf x},\\Theta_1),\\Theta_2),\\Theta_3) & D_{\\Theta_3} f_3(f_2(f_1({\\bf x},\\Theta_1),\\Theta_2),\\Theta_3) & {\\bf 0}\\\\\n",
    "{\\bf 0} & {\\bf 0} & I\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "D_{{\\bf x}_2} f_2(f_1({\\bf x},\\Theta_1),\\Theta_2) & D_{\\Theta_2} f_2(f_1({\\bf x},\\Theta_1),\\Theta_2) & {\\bf 0} & {\\bf 0}\\\\\n",
    "{\\bf 0} & {\\bf 0} & I & {\\bf 0}\\\\\n",
    "{\\bf 0} & {\\bf 0} & {\\bf 0} & I\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "D_{\\Theta_1} f_1({\\bf x},\\Theta_1) & {\\bf 0} & {\\bf 0} & {\\bf 0}\\\\\n",
    "{\\bf 0} & I & {\\bf 0} & {\\bf 0}\\\\\n",
    "{\\bf 0} & {\\bf 0} & I & {\\bf 0}\\\\\n",
    "{\\bf 0} & {\\bf 0} & {\\bf 0} & I\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Therefore we can represent the gradient in the block form\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\left(D_{\\Theta_1}\\: f_1\\right)^T \\left(D_{{\\bf x}_2}\\: f_2\\right)^T \\left(D_{{\\bf x}_3}\\: f_3\\right)^T\\nabla_{{\\bf x}_4}\\: f_4\\\\\n",
    "\\left(D_{\\Theta_2}\\: f_2\\right)^T \\left(D_{{\\bf x}_3}\\: f_3\\right)^T\\nabla_{{\\bf x}_4}\\: f_4\\\\\n",
    "\\left(D_{\\Theta_3}\\: f_3\\right)^T\\nabla_{{\\bf x}_4}\\: f_4\\\\\n",
    "\\nabla_{\\Theta_4}\\: f_4\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "The most general form of backpropagation can be explained in terms of tensor contraction. \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

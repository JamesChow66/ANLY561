{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block Jacobians and the Chain Rule\n",
    "\n",
    "Suppose $f\\in C^1\\left(\\mathcal{T}_{{\\bf n}_1}\\times\\mathcal{T}_{{\\bf n}_2};\\mathcal{T}_{{\\bf k}_1}\\right)$, $g\\in C^1\\left(\\mathcal{T}_{{\\bf m}_1}\\times\\mathcal{T}_{{\\bf m}_2};\\mathcal{T}_{{\\bf k}_2}\\right)$, and $h\\in C^1\\left(\\mathcal{T}_{{\\bf k}_1}\\times\\mathcal{T}_{{\\bf k}_2}, \\mathcal{T}_{{\\bf l}}\\right)$, then the function $\\varphi(\\mathcal{V}, \\mathcal{W},\\mathcal{X},\\mathcal{Y}) = h(f(\\mathcal{V},\\mathcal{W}),g(\\mathcal{X},\\mathcal{Y}))$ satisfies $\\varphi\\in C^1\\left(\\mathcal{T}_{{\\bf n}_1}\\times\\mathcal{T}_{{\\bf n}_2}\\times\\mathcal{T}_{{\\bf m}_1}\\times\\mathcal{T}_{{\\bf m}_2};\\mathcal{T}_{\\bf l}\\right)$. Because of the Cartesian product, we can't write down a single tensor for our Jacobian. Instead, we can get Jacobians for each block of variables:\n",
    "$$\n",
    "D_{\\mathcal{V}}\\varphi(\\mathcal{V}, \\mathcal{W},\\mathcal{X},\\mathcal{Y})\\in \\mathcal{T}_{{\\bf l}\\oplus{\\bf n}_1},\n",
    "$$\n",
    "$$\n",
    "D_{\\mathcal{W}}\\varphi(\\mathcal{V}, \\mathcal{W},\\mathcal{X},\\mathcal{Y})\\in \\mathcal{T}_{{\\bf l}\\oplus{\\bf n}_2},\n",
    "$$\n",
    "$$\n",
    "D_{\\mathcal{X}}\\varphi(\\mathcal{V}, \\mathcal{W},\\mathcal{X},\\mathcal{Y})\\in \\mathcal{T}_{{\\bf l}\\oplus{\\bf m}_1},\n",
    "$$\n",
    "and\n",
    "$$\n",
    "D_{\\mathcal{Y}}\\varphi(\\mathcal{V}, \\mathcal{W},\\mathcal{X},\\mathcal{Y})\\in \\mathcal{T}_{{\\bf l}\\oplus{\\bf m}_2},\n",
    "$$\n",
    "Thinking of $h$ as $h(\\mathcal{F},\\mathcal{G})$, we also have the Jacobian blocks\n",
    "$$\n",
    "D_{\\mathcal{F}} h(\\mathcal{F},\\mathcal{G})\\in \\mathcal{T}_{{\\bf l}\\oplus{\\bf k}_1},\\: D_{\\mathcal{G}} h(\\mathcal{F},\\mathcal{G})\\in \\mathcal{T}_{{\\bf l}\\oplus{\\bf k}_2},\n",
    "$$\n",
    "$$\n",
    "D_{\\mathcal{V}} f(\\mathcal{V},\\mathcal{W})\\in \\mathcal{T}_{{\\bf k}_1\\oplus{\\bf n}_1},\\: D_{\\mathcal{W}} f(\\mathcal{V},\\mathcal{W})\\in \\mathcal{T}_{{\\bf k}_1\\oplus{\\bf n}_2},\n",
    "$$\n",
    "$$\n",
    "D_{\\mathcal{X}} g(\\mathcal{X},\\mathcal{Y})\\in \\mathcal{T}_{{\\bf k}_2\\oplus{\\bf m}_1},\\: D_{\\mathcal{Y}} g(\\mathcal{X},\\mathcal{Y})\\in \\mathcal{T}_{{\\bf k}_2\\oplus{\\bf m}_2}\n",
    "$$\n",
    "The chain rule in this block formulation is then (suppressing arguments)\n",
    "$$\n",
    "D_{\\mathcal{V}}\\varphi_{({\\bf i},{\\bf j})} = c(D_{\\mathcal{F}}h, D_{\\mathcal{V}} f)_{({\\bf i},{\\bf j})}=\\left(\\sum_{\\bf k} \\frac{\\partial h_{\\bf i}}{\\partial f_{\\bf k}} \\frac{\\partial f_{\\bf k}}{\\partial v_{\\bf j}}\\right)_{({\\bf i},{\\bf j})}\n",
    "$$\n",
    "$$\n",
    "D_{\\mathcal{W}}\\varphi_{({\\bf i},{\\bf j})} = c(D_{\\mathcal{F}}h, D_{\\mathcal{W}} f)_{({\\bf i},{\\bf j})}=\\left(\\sum_{\\bf k} \\frac{\\partial h_{\\bf i}}{\\partial f_{\\bf k}} \\frac{\\partial f_{\\bf k}}{\\partial w_{\\bf j}}\\right)_{({\\bf i},{\\bf j})}\n",
    "$$\n",
    "$$\n",
    "D_{\\mathcal{X}}\\varphi_{({\\bf i},{\\bf j})} = c(D_{\\mathcal{G}}h, D_{\\mathcal{X}} g)_{({\\bf i},{\\bf j})}=\\left(\\sum_{\\bf k} \\frac{\\partial h_{\\bf i}}{\\partial g_{\\bf k}} \\frac{\\partial g_{\\bf k}}{\\partial x_{\\bf j}}\\right)_{({\\bf i},{\\bf j})}\n",
    "$$\n",
    "$$\n",
    "D_{\\mathcal{Y}}\\varphi_{({\\bf i},{\\bf j})} = c(D_{\\mathcal{G}}h, D_{\\mathcal{Y}} g)_{({\\bf i},{\\bf j})}=\\left(\\sum_{\\bf k} \\frac{\\partial h_{\\bf i}}{\\partial g_{\\bf k}} \\frac{\\partial g_{\\bf k}}{\\partial y_{\\bf j}}\\right)_{({\\bf i},{\\bf j})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "\n",
    "Consider a feedforward neural network with a single hidden layer. The objective function may be written as\n",
    "$$\n",
    "\\sum_{i=1}^N f_2({\\bf y}^{(i)}, f_1({\\bf x}^{(i)}; W, {\\bf b}); V, {\\bf c})\n",
    "$$\n",
    "for $W\\in \\mathcal{T}_{(k,d)}, {\\bf b}\\in \\mathcal{T}_{(k)}, V\\in\\mathcal{T}_{(k, m)}, {\\bf c}\\in\\mathcal{T}_{(m)}$. We will compute the gradient in blocks. First, we note that\n",
    "$$\n",
    "\\nabla_W\\sum_{i=1}^N f_2({\\bf y}^{(i)}, f_1({\\bf x}^{(i)}; W, {\\bf b}); V, {\\bf c})=\\sum_{i=1}^N \\nabla_W f_2({\\bf y}^{(i)}, f_1({\\bf x}^{(i)}; W, {\\bf b}); V, {\\bf c}).\n",
    "$$\n",
    "Moreover, the gradient is simply the transpose of the Jacobian, so we will simply compute\n",
    "$$\n",
    "D_W f_2({\\bf y}, f_1({\\bf x}; W, {\\bf b}); V, {\\bf c}).\n",
    "$$\n",
    "Viewing $f_2$ as $f_2({\\bf y},\\xi; V, {\\bf c})$, we then have that this Jacobian is the standard contraction of $D_\\xi f_2({\\bf y}, f_1({\\bf x}; W, {\\bf b}); V, {\\bf c})$ with $D_W f_1({\\bf x}; W, {\\bf b})$. Similarly, \n",
    "$$\n",
    "D_{\\bf b} f_2({\\bf y}, f_1({\\bf x}; W, {\\bf b}); V, {\\bf c})\n",
    "$$\n",
    "is the contraction of $D_\\xi f_2({\\bf y}, f_1({\\bf x}; W, {\\bf b}); V, {\\bf c})$ with $D_{\\bf b} f_1({\\bf x}; W, {\\bf b})$. Because $V$ and ${\\bf c}$ do not factor through compositions, their blocks are computable without the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute the Jacobian of X @ W wrt W\n",
    "def affine_jacobian(X, W):\n",
    "    # (d_{i, j} (X @ W))_{a, b} = e_a^T X e_ie_j^T e_b\n",
    "    D = np.zeros((X.shape[0], W.shape[1], W.shape[0], W.shape[1]))\n",
    "    for k in range(W.shape[1]):\n",
    "        D[:,k,:,k]=X\n",
    "    return D\n",
    "    \n",
    "def logit(z):\n",
    "    # This is vectorized\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def Dlogit(Z):\n",
    "    # The Jacobian of the matrix logit\n",
    "    D = np.zeros((Z.shape[0], Z.shape[1], Z.shape[0], Z.shape[1]))\n",
    "    for k in range(Z.shape[0]):\n",
    "        D[k,:,k,:] = np.diag(logit(Z[k,:])*logit(-Z[k,:]))\n",
    "    return D\n",
    "\n",
    "def softmax(z):\n",
    "    v = np.exp(z)\n",
    "    return v / np.sum(v)\n",
    "\n",
    "def matrix_softmax(Z):\n",
    "    return np.apply_along_axis(softmax, 1, Z)\n",
    "\n",
    "def Dmatrix_softmax(Z):\n",
    "    D = np.zeros((Z.shape[0], Z.shape[1], Z.shape[0], Z.shape[1]))\n",
    "    for k in range(Z.shape[0]):\n",
    "        v = np.exp(Z[k,:])\n",
    "        v = v / np.sum(v)\n",
    "        D[k,:,k,:] = np.diag(v) - v @ v.T\n",
    "    return D\n",
    "\n",
    "def cross_entropy(P, Q):\n",
    "    return -np.sum(P * np.log(Q))\n",
    "\n",
    "def DQcross_entropy(P, Q):\n",
    "    return - P @ (1/Q)\n",
    "\n",
    "def nn_loss_closure(X, Y):\n",
    "    # vars[0]=W, vars[1]=b, vars[2]=V, vars[3]=c\n",
    "    def f(vars):\n",
    "        return cross_entropy(Y, matrix_softmax(logit(X @ vars[0] + vars[1]) @ vars[2] + vars[3]))\n",
    "    return f\n",
    "\n",
    "def nn_loss_gradient_closure(X, Y):\n",
    "    def df(vars):\n",
    "        # Gather all the intermediate Jacobians\n",
    "        XWb = X @ vars[0] + vars[1]\n",
    "        H = logit(XWb)\n",
    "        Hvc = H @ vars[2] + vars[3]\n",
    "        DQ = DQcross_entropy(Y, matrix_softmax(HVc))\n",
    "        DZ = Dmatrix_softmax(HVc)\n",
    "        DV = affine_jacobian(H, vars[2])\n",
    "        Dc = affine_jacobian(np.ones(c.shape[1]).T, c)\n",
    "        DXWb = Dlogit(XWb)\n",
    "        DW = affine_jacobian(X, vars[0])\n",
    "        Dc = affine_jacobian(np.ones(b.shape[1]).T, b)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Problems\n",
    "\n",
    "Finish the nn_loss_gradient closure code using the chain rule and np.tensordot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Consider the functions $f_1, f_2, f_3:\\mathbb{R}^2\\rightarrow\\mathbb{R}$ as $f_1(x_1,\\theta_1)$, $f_2(x_2,\\theta_2)$, and $f_3(x_3,\\theta_3)$. Then define\n",
    "\n",
    "$$\n",
    "g(x; \\theta_1, \\theta_2, \\theta_3) = f_3(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3).\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\frac{\\partial g}{\\partial\\theta_1}(x;\\theta_1, \\theta_2, \\theta_3)=\\frac{\\partial f_3}{\\partial x_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3)\\frac{\\partial}{\\partial\\theta_1}\\left[f_2(f_1(x,\\theta_1),\\theta_2)\\right] + \\frac{\\partial f_3}{\\partial \\theta_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3)\\frac{\\partial \\theta_3}{\\partial\\theta_1}=\\frac{\\partial f_3}{\\partial x_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3)\\left[\\frac{\\partial f_2}{\\partial x_2}(f_1(x,\\theta_1),\\theta_2)\\frac{\\partial f_1}{\\partial\\theta_1}(x,\\theta_1) + \\frac{\\partial f_2}{\\partial \\theta_2}(f_1(x,\\theta_1),\\theta_2)\\frac{\\partial \\theta_2}{\\partial \\theta_1}\\right] = \\frac{\\partial f_3}{\\partial x_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3)\\frac{\\partial f_2}{\\partial x_2}(f_1(x,\\theta_1),\\theta_2)\\frac{\\partial f_1}{\\partial\\theta_1}(x,\\theta_1).\n",
    "$$\n",
    "\n",
    "Similarly,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial g}{\\partial\\theta_2}(x;\\theta_1, \\theta_2, \\theta_3) = \\frac{\\partial f_3}{\\partial x_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3)\\frac{\\partial f_2}{\\partial \\theta_2}(f_1(x,\\theta_1),\\theta_2)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{\\partial g}{\\partial\\theta_3}(x;\\theta_1, \\theta_2, \\theta_3) = \\frac{\\partial f_3}{\\partial \\theta_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3).\n",
    "$$\n",
    "\n",
    "Another way to see this is to view this as the sequence of maps\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\theta_1\\\\\n",
    "\\theta_2\\\\\n",
    "\\theta_3\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_1(x,\\theta_1)\\\\\n",
    "\\theta_2\\\\\n",
    "\\theta_3\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_2(f_1(x,\\theta_1),\\theta_2)\\\\\n",
    "\\theta_3\n",
    "\\end{pmatrix}\\longmapsto f_3(f_2(f_1(x,\\theta_1),\\theta_2)\\theta_3).\n",
    "$$\n",
    "\n",
    "The Jacobians of these maps are\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial f_1}{\\partial\\theta_1}(x,\\theta_1) & 0 & 0\\\\\n",
    "0 & 1 & 0\\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}, \\begin{pmatrix}\n",
    "\\frac{\\partial f_2}{\\partial x_2}(f_1(x,\\theta_1),\\theta_2) & \\frac{\\partial f_2}{\\partial \\theta_2}(f_1(x,\\theta_1),\\theta_2) & 0\\\\\n",
    "0 & 0 & 1\\\\\n",
    "\\end{pmatrix},\\text{ and } \\begin{pmatrix}\n",
    "\\frac{\\partial f_3}{\\partial x_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3) & \\frac{\\partial f_3}{\\partial \\theta_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The interesting thing to note here is that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_3}{\\partial x_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3)\n",
    "$$\n",
    "\n",
    "is a factor for two of these partial derivatives. This redundancy is more pronounced as we get a deeper composition. Consider the composition of maps\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\theta_1\\\\\n",
    "\\theta_2\\\\\n",
    "\\theta_3\\\\\n",
    "\\theta_4\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_1(x,\\theta_1)\\\\\n",
    "\\theta_2\\\\\n",
    "\\theta_3\\\\\n",
    "\\theta_4\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_2(f_1(x,\\theta_1),\\theta_2)\\\\\n",
    "\\theta_3\\\\\n",
    "\\theta_4\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_3(f_2(f_1(x,\\theta_1),\\theta_2)\\theta_3)\\\\\n",
    "\\theta_4\n",
    "\\end{pmatrix}\\longmapsto f_4(f_3(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3),\\theta_4).\n",
    "$$\n",
    "\n",
    "The Jacobian of this composition (by the chain rule)\n",
    "\n",
    "$$\n",
    "\\tiny\\begin{pmatrix}\n",
    "\\frac{\\partial f_4}{\\partial x_4}(f_3(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3), \\theta_4) & \\frac{\\partial f_4}{\\partial \\theta_4}(f_3(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3), \\theta_4)\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "\\frac{\\partial f_3}{\\partial x_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3) & \\frac{\\partial f_3}{\\partial \\theta_3}(f_2(f_1(x,\\theta_1),\\theta_2),\\theta_3) & 0\\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "\\frac{\\partial f_2}{\\partial x_2}(f_1(x,\\theta_1),\\theta_2) & \\frac{\\partial f_2}{\\partial \\theta_2}(f_1(x,\\theta_1),\\theta_2) & 0 & 0\\\\\n",
    "0 & 0 & 1 & 0\\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "\\frac{\\partial f_1}{\\partial\\theta_1}(x,\\theta_1) & 0 & 0 & 0\\\\\n",
    "0 & 1 & 0 & 0\\\\\n",
    "0 & 0 & 1 & 0\\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{pmatrix}, \n",
    "$$\n",
    "\n",
    "This means that the gradient has the form (with suppression of arguments):\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial f_4}{\\partial x_4} \\frac{\\partial f_3}{\\partial x_3} \\frac{\\partial f_2}{\\partial x_2} \\frac{\\partial f_1}{\\partial \\theta_1}\\\\\n",
    "\\frac{\\partial f_4}{\\partial x_4} \\frac{\\partial f_3}{\\partial x_3} \\frac{\\partial f_2}{\\partial \\theta_2}\\\\\n",
    "\\frac{\\partial f_4}{\\partial x_4} \\frac{\\partial f_3}{\\partial \\theta_3}\\\\\n",
    "\\frac{\\partial f_4}{\\partial \\theta_4}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "This suggests the following computational structure for computing a gradient descent update using step size $\\eta>0$:\n",
    "\n",
    "1. $\\theta_4^\\prime = \\theta_4 - \\eta \\frac{\\partial f_4}{\\partial \\theta_4}$ and set $q=\\frac{\\partial f_4}{\\partial x_4}$.\n",
    "2. For $i=3, 2, 1$: set $\\theta_i^\\prime = \\theta_i -\\eta q \\frac{\\partial f_i}{\\partial \\theta_i}$ and $q= q \\frac{\\partial f_i}{\\partial x_i}$\n",
    "\n",
    "This is a simplified version of the **backpropagation** algorithm (or, backprop).  Now, let's suppose that $f_1({\\bf x}_1, \\Theta_1)$, $f_2({\\bf x}_2, \\Theta_2)$, $f_3({\\bf x}_3,\\Theta_3)$, and $f_4({\\bf x}_4, \\Theta_4)$ where the ${\\bf x}$'s and $\\Theta$'s are vectors of parameters. Then our composition has a *block form* given by\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\Theta_1\\\\\n",
    "\\Theta_2\\\\\n",
    "\\Theta_3\\\\\n",
    "\\Theta_4\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_1({\\bf x},\\Theta_1)\\\\\n",
    "\\Theta_2\\\\\n",
    "\\Theta_3\\\\\n",
    "\\Theta_4\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_2(f_1({\\bf x},\\Theta_1),\\Theta_2)\\\\\n",
    "\\Theta_3\\\\\n",
    "\\Theta_4\n",
    "\\end{pmatrix}\\longmapsto \\begin{pmatrix}\n",
    "f_3(f_2(f_1({\\bf x},\\Theta_1),\\Theta_2)\\Theta_3)\\\\\n",
    "\\Theta_4\n",
    "\\end{pmatrix}\\longmapsto f_4(f_3(f_2(f_1({\\bf x},\\Theta_1),\\Theta_2),\\Theta_3),\\Theta_4).\n",
    "$$\n",
    "\n",
    "and the chain rule gives the Jacobian (in block form)\n",
    "\n",
    "$$\n",
    "\\tiny\\begin{pmatrix}\n",
    "D_{{\\bf x}_4}f_4(f_3(f_2(f_1({\\bf x},\\Theta_1),\\Theta_2),\\Theta_3), \\Theta_4) & D_{\\Theta_4}f_4(f_3(f_2(f_1({\\bf x},\\Theta_1),\\Theta_2),\\Theta_3), \\Theta_4)\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "D_{{\\bf x}_3} f_3(f_2(f_1({\\bf x},\\Theta_1),\\Theta_2),\\Theta_3) & D_{\\Theta_3} f_3(f_2(f_1({\\bf x},\\Theta_1),\\Theta_2),\\Theta_3) & {\\bf 0}\\\\\n",
    "{\\bf 0} & {\\bf 0} & I\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "D_{{\\bf x}_2} f_2(f_1({\\bf x},\\Theta_1),\\Theta_2) & D_{\\Theta_2} f_2(f_1({\\bf x},\\Theta_1),\\Theta_2) & {\\bf 0} & {\\bf 0}\\\\\n",
    "{\\bf 0} & {\\bf 0} & I & {\\bf 0}\\\\\n",
    "{\\bf 0} & {\\bf 0} & {\\bf 0} & I\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "D_{\\Theta_1} f_1({\\bf x},\\Theta_1) & {\\bf 0} & {\\bf 0} & {\\bf 0}\\\\\n",
    "{\\bf 0} & I & {\\bf 0} & {\\bf 0}\\\\\n",
    "{\\bf 0} & {\\bf 0} & I & {\\bf 0}\\\\\n",
    "{\\bf 0} & {\\bf 0} & {\\bf 0} & I\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Therefore we can represent the gradient in the block form\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\left(D_{\\Theta_1}\\: f_1\\right)^T \\left(D_{{\\bf x}_2}\\: f_2\\right)^T \\left(D_{{\\bf x}_3}\\: f_3\\right)^T\\nabla_{{\\bf x}_4}\\: f_4\\\\\n",
    "\\left(D_{\\Theta_2}\\: f_2\\right)^T \\left(D_{{\\bf x}_3}\\: f_3\\right)^T\\nabla_{{\\bf x}_4}\\: f_4\\\\\n",
    "\\left(D_{\\Theta_3}\\: f_3\\right)^T\\nabla_{{\\bf x}_4}\\: f_4\\\\\n",
    "\\nabla_{\\Theta_4}\\: f_4\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
